{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 5 - Automatic summarization\n",
    "\n",
    "Si vuole ridurre un documento del 10%, 20% o 30% secondo la seguente strategia\n",
    "\n",
    "1. Individuare l'argomento del testo che si sta riassumendo; l'argomento può essere indicato come un (insieme di) vettori NASARI:\n",
    "    - vt1 = {term1_score, term2_score, …, term10_score }\n",
    "    - vt2 = {term1_score, term2_score, …, term10_score } \n",
    "    - ...\n",
    "\n",
    "2. creare il contesto, raccogliendo qui i vettori dei termini (questo passaggio può essere ripetuto, scaricando ad ogni round il contributo dei termini associati)\n",
    "\n",
    "3. conservare i paragrafi le cui frasi contengono i termini più salienti, in base alla sovrapposizione ponderata, WO(v1,v2)\n",
    "    - riclassificare il peso dei paragrafi applicando almeno uno degli approcci menzionati (titolo, spunto, frase, coesione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "#BABELNET_TOKEN = '1e258739-f5e4-4961-8267-a2da4fe94572' #MO\n",
    "BABELNET_TOKEN = '01a5d861-2f36-45cb-8974-a2a6526530d2' #LT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Metodo utilizzato per eseguire il preprocessing delle frasi, in cui vengono effettuate le seguenti operazioni:\n",
    "- Rimozione della punteggiatura\n",
    "- Trasformazione delle lettere in lowercase\n",
    "- Tokenizzazione della frase tenendo conte delle multiword expression\n",
    "- Lemmatizzazione di tutte le parole\n",
    "- Rimozione delle stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #remove stop words\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_processing(document):\n",
    "    document = re.sub(r'[^\\w\\s]',' ',document) #remove punctuation\n",
    "    document = document.lower()\n",
    "    document = tokenizer.tokenize(document.split())\n",
    "    document = [lemmatizer.lemmatize(token) for token in document]  \n",
    "    document = [w for w in document if not w in stop_words]\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babelnet Id di una frase\n",
    "\n",
    "Viene utilizzato principalmente per ottenere i Babelnet Id delle parole del titolo, che una volta sottosposti a WSD (in quanto, molto probabilmente, per ogni parola avremo più synset) ci serviranno per ottenere i vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_babelnet_synset_by_word(word):\n",
    "    response = requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "    print('=============')\n",
    "    print(response)\n",
    "    print('=============')\n",
    "\n",
    "    name = response['senses'][0]['properties']['fullLemma']\n",
    "    glosses = \"\"\n",
    "    for gloss in response['glosses']:\n",
    "        glosses += str(gloss['gloss']) + ';'\n",
    "    examples = \"\"\n",
    "    for example in response['examples']:\n",
    "        examples += str(example['example']) + ';'\n",
    "\n",
    "    #add row to df and save it\n",
    "    df = df.concat({'id': syn_id, 'name': name, 'glosses': glosses, 'examples': examples}, ignore_index=True)\n",
    "    df.to_csv('data/local_babelnet_syns.csv', index=False)\n",
    "'''\n",
    "Data una frase restituisce tutti i suoi babelnet id\n",
    "'''\n",
    "def get_sentence_babelnet_ids(file_name, sentence):\n",
    "    if os.path.exists('data/ids-'+ file_name +'.json'):\n",
    "        with open('data/ids-'+ file_name +'.json') as json_file:\n",
    "            ids = json.load(json_file)\n",
    "    else:\n",
    "        ids = {}\n",
    "        # prendo gli id di babelnet per ogni parola della frase\n",
    "        for word in sentence:\n",
    "            ids[word] = requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "        # prendo i synset di babelnet per ogni parola della frase\n",
    "        with open('data/ids_'+ file_name +'.json', 'w') as fp:\n",
    "            json.dump(ids, fp)\n",
    "\n",
    "    return ids\n",
    "\n",
    "'''\n",
    "Dato un babelnet id guardo nel file locale se ho già le informazioni, altrimenti \n",
    "faccio una richiesta a babelnet e aggiungo la riga al file locale\n",
    "'''\n",
    "def get_babelnet_synset_by_id(syn_id):\n",
    "    df = pd.read_csv('data/local_babelnet_syns.csv')\n",
    "    glosses = ['']\n",
    "    examples = ['']\n",
    "\n",
    "    if syn_id in df['id'].values:\n",
    "        row = df[df['id'] == syn_id]\n",
    "        name = row['name'].values[0]\n",
    "        if not 'nan' in str(row['glosses'].values[0]):\n",
    "            glosses = row['glosses'].values[0].split(';')\n",
    "        if not 'nan' in str(row['examples'].values[0]):\n",
    "            examples = row['examples'].values[0].split(';')             \n",
    "    else:\n",
    "        response = requests.get(f'https://babelnet.io/v8/getSynset?id={syn_id}&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "\n",
    "        print('=============')\n",
    "        print(response)\n",
    "        print('=============')\n",
    "\n",
    "        name = response['senses'][0]['properties']['fullLemma']\n",
    "        glosses = \"\"\n",
    "        for gloss in response['glosses']:\n",
    "            glosses += str(gloss['gloss']) + ';'\n",
    "        examples = \"\"\n",
    "        for example in response['examples']:\n",
    "            examples += str(example['example']) + ';'\n",
    "\n",
    "        #add row to df and save it\n",
    "        df = df.concat({'id': syn_id, 'name': name, 'glosses': glosses, 'examples': examples}, ignore_index=True)\n",
    "        df.to_csv('data/local_babelnet_syns.csv', index=False)\n",
    "\n",
    "        glosses = glosses.split(';')\n",
    "        examples = examples.split(';')\n",
    "\n",
    "    return syn_id, name, glosses, examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasari\n",
    "\n",
    "Estrazione dei vettori Nasari dal file locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors():\n",
    "    nasari_vectors = pd.read_csv('data/dd-nasari.txt', on_bad_lines='skip', header=None, sep=';')\n",
    "    nasari_vectors = nasari_vectors.set_index(0)\n",
    "    return nasari_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Simplified Lesk\n",
    "\n",
    "Mi server per fare il WSD dei synsets ottenuti dei token del titolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dati i tutti i synset di una parola e il suo contesto, andando ad applicare\n",
    "il Lesk, restituisce il synset con il contesto più simile\n",
    "'''\n",
    "def get_signature(bn_syn):\n",
    "    _, _, glosses, examples = get_babelnet_synset_by_id(bn_syn)\n",
    "\n",
    "    signature = \"\"\n",
    "    for gloss in glosses:\n",
    "        signature += gloss + ' '\n",
    "    for example in examples:\n",
    "        signature += example + ' '\n",
    "    return set(pre_processing(signature))\n",
    "\n",
    "def simplified_lesk(word, context):\n",
    "    nasari_vectors = get_nasari_vectors_by_token(word)\n",
    "    for bn_syn in bn_syns:\n",
    "        signature = get_signature(bn_syn['id'])\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = bn_syn['id']\n",
    "    return best_sense\n",
    "\n",
    "# Usa come contesto l'intero testo del file, non solo il titolo\n",
    "def simplified_lesk(bn_syns, context):\n",
    "    best_sense = None\n",
    "    if bn_syns is not None and len(bn_syns) > 0:\n",
    "        print(bn_syns)\n",
    "        best_sense = bn_syns[0]['id']\n",
    "        max_overlap = 0\n",
    "\n",
    "        for bn_syn in bn_syns:\n",
    "            signature = get_signature(bn_syn['id'])\n",
    "            overlap = len(context.intersection(signature))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = bn_syn['id']\n",
    "    return best_sense\n",
    "\n",
    "def get_best_senses(ids, context):\n",
    "    senses = {}\n",
    "    for word in ids:\n",
    "        senses[word] = simplified_lesk(ids[word], context)\n",
    "\n",
    "    return senses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del documento da tradurre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(file_name):\n",
    "    text = open('data/docs/' + file_name + '.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    # pre processing\n",
    "    text = [line for line in text if line != '']\n",
    "    text_preprocessed = [pre_processing(line) for line in text[0:]]\n",
    "\n",
    "    # prendo il contesto per fare WSD dei babelnet id delle parole del testo\n",
    "    context_for_wsd = []\n",
    "    for sentence in text_preprocessed:\n",
    "        context_for_wsd = context_for_wsd + sentence\n",
    "    context_for_wsd = set(context_for_wsd)\n",
    "\n",
    "    # prendo il titolo\n",
    "    title_preprocessed = text_preprocessed[0]\n",
    "\n",
    "    return title_preprocessed, text_preprocessed, text, context_for_wsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associazione Vectors - Synsets\n",
    "\n",
    "Associazione dei vettori Nasari ai Synset disambiguati delle parole del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors_by_senses(senses):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    vectors = {}\n",
    "\n",
    "    for word in senses:\n",
    "        if senses[word] in list(nasari_vectors.index.values):\n",
    "            #print(f'{senses[word]} in nasari_vectors') \n",
    "            vectors[word] = nasari_vectors.loc[senses[word]]\n",
    "        #else:\n",
    "            #print(f'{senses[word]} not in nasari_vectors')\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "def get_nasari_vectors_by_token(word):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    nasari_vectors[0].fillna('', inplace=True)\n",
    "    print(nasari_vectors)\n",
    "    return nasari_vectors[nasari_vectors[0].str.contains(word)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "def get_contexts_from_sentence_word():\n",
    "    context = {}\n",
    "    for index, row in get_nasari_vectors_by_token('Tuple').iterrows():\n",
    "        key = row.name\n",
    "        row_values = row.iloc[1:].apply(lambda x: x.split('_')[0] if len(x.split('_')) > 0 else '').values\n",
    "        concatenated_row = ' '.join(row_values)\n",
    "        context[key] = concatenated_row\n",
    "    print(context)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          1                2   \\\n",
      "0                                                               \n",
      "bn:00000002n                       The Hague     hague_1372.4   \n",
      "bn:00000003n                  .22 Long Rifle    rifle_2305.63   \n",
      "bn:00000005n                           Tuple     number_754.0   \n",
      "bn:00000006n                       Dodecanol   alcohol_310.46   \n",
      "bn:00000013n                         Million   million_209.35   \n",
      "...                                      ...              ...   \n",
      "bn:17368592n                   Leiningerland    eckbach_91.81   \n",
      "bn:17368597n       As Above, So Below (film)    flamel_120.74   \n",
      "bn:17368601n                       Repeaters      sonia_42.02   \n",
      "bn:17368602n       2013 Uzbekistan Super Cup  bunyodkor_96.28   \n",
      "bn:17368606n  Old Mother Riley's New Venture      lucan_74.04   \n",
      "\n",
      "                             3                    4                    5   \\\n",
      "0                                                                           \n",
      "bn:00000002n   rotterdam_427.35     amsterdam_415.35           city_218.8   \n",
      "bn:00000003n  cartridge_2279.59       bullet_1365.01        barrel_957.51   \n",
      "bn:00000005n     integer_528.26            set_449.2         tuple_373.74   \n",
      "bn:00000006n      ethanol_74.72      dodecanol_45.46             ch_44.51   \n",
      "bn:00000013n      number_146.31     mathematics_61.3     long scale_53.31   \n",
      "...                         ...                  ...                  ...   \n",
      "bn:17368592n       castle_88.94  leiningerland_85.56   neuleiningen_68.67   \n",
      "bn:17368597n        stone_59.17    philosopher_30.49        scarlett_26.6   \n",
      "bn:17368601n      michael_35.29           kyle_35.05  groundhog day_24.84   \n",
      "bn:17368602n  uzbek league_49.7     uzbekistan_30.84       uzbek cup_28.4   \n",
      "bn:17368606n        riley_64.41        mcshane_43.82         mother_30.58   \n",
      "\n",
      "                                    6                   7   \\\n",
      "0                                                            \n",
      "bn:00000002n         netherlands_198.2        dutch_197.61   \n",
      "bn:00000003n            firearm_910.38       shotgun_702.1   \n",
      "bn:00000005n            element_323.63       tuples_316.55   \n",
      "bn:00000006n               fatty_35.93            oh_33.61   \n",
      "bn:00000013n         real number_50.43       numeral_50.35   \n",
      "...                                ...                 ...   \n",
      "bn:17368592n        altleiningen_55.55     leiningen_42.25   \n",
      "bn:17368597n            catacomb_26.01       papillon_20.0   \n",
      "bn:17368601n                star_20.27       halsted_14.75   \n",
      "bn:17368602n  lokomotiv tashkent_24.61      supercup_22.74   \n",
      "bn:17368606n    old mother riley_29.54  arthur lucan_29.02   \n",
      "\n",
      "                                 8                       9   \\\n",
      "0                                                             \n",
      "bn:00000002n         utrecht_186.22        the hague_160.32   \n",
      "bn:00000003n         rimfire_563.73          caliber_535.97   \n",
      "bn:00000005n          define_272.01         function_271.05   \n",
      "bn:00000006n           carbon_33.53          aldehyde_32.11   \n",
      "bn:00000013n      short scale_50.12             digit_42.17   \n",
      "...                             ...                     ...   \n",
      "bn:17368592n        grünstadt_32.28  hettenleidelheim_28.79   \n",
      "bn:17368597n       alchemical_19.62    françois civil_16.33   \n",
      "bn:17368601n  dustin milligan_14.28              klerk_11.4   \n",
      "bn:17368602n             2013_20.44             match_18.91   \n",
      "bn:17368606n              old_24.63     kitty mcshane_24.33   \n",
      "\n",
      "                               10  \n",
      "0                                  \n",
      "bn:00000002n          page_137.18  \n",
      "bn:00000003n     ammunition_474.7  \n",
      "bn:00000005n   permutation_269.38  \n",
      "bn:00000006n       methanol_31.53  \n",
      "bn:00000013n          bally_41.77  \n",
      "...                           ...  \n",
      "bn:17368592n            jpg_28.66  \n",
      "bn:17368597n  perdita weeks_14.51  \n",
      "bn:17368601n           film_11.22  \n",
      "bn:17368602n      runner-up_17.59  \n",
      "bn:17368606n          kitty_18.94  \n",
      "\n",
      "[3568562 rows x 10 columns]\n",
      "{'bn:00000005n': 'number integer set tuple element tuples define function permutation', 'bn:00028343n': 'tuple set tuples relation assassination film permutation -lcb- function', 'bn:00065519n': 'set tuple relation element function tuples define permutation -lcb-', 'bn:00078196n': 'set tuple relation element function tuples define permutation -lcb-', 'bn:00252664n': 'column1 enddate table historytable relational database tuple columnn column2', 'bn:00607504n': 'xtuple openmfg erp postbooks web client database functionality gui', 'bn:00993128n': 'database sql datum query model tuples relational dbms data', 'bn:01235815n': 'api node object datum javaspaces javaspace tuple space tuples', 'bn:03367405n': 'rhythm beat triplet quarter note time signature meter tempo cross-rhythm eighth note', 'bn:15410128n': 'dependency xn x1 tgd zk -lcb- y1 ym relational'}\n"
     ]
    }
   ],
   "source": [
    "get_contexts_from_sentence_word()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "def get_nasari_vector_by_sense(sense):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    vector = None\n",
    "    if sense in list(nasari_vectors.index.values):\n",
    "        vector = nasari_vectors.loc[sense]\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del contesto per fare text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_context(vectors):\n",
    "    get_weighted_context = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        #print(vectors[vector])\n",
    "        sum_weights = 0\n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:\n",
    "                sum_weights += float(word.split('_')[1])\n",
    "            \n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:    \n",
    "                weight = float(word.split('_')[1]) / sum_weights\n",
    "                # preprocess word\n",
    "                word_to_added = word.split('_')[0]\n",
    "                word_to_added = re.sub(r'[^\\w\\s]',' ',word_to_added) #remove punctuation\n",
    "                word_to_added = word_to_added.lower()\n",
    "                word_to_added = lemmatizer.lemmatize(word_to_added)\n",
    "\n",
    "                get_weighted_context.append((word_to_added, weight))\n",
    "\n",
    "    return get_weighted_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesatura dei paragrafi\n",
    "\n",
    "Eseguita usando la metrica Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "# weighted overlap\n",
    "def weighted_overlap2(vect1, vect2):\n",
    "    tot = 0.0\n",
    "    overlap = 0\n",
    "    for i, elem in enumerate(vect1):\n",
    "        try:\n",
    "            index = vect2.index(elem) + 1\n",
    "            overlap += 1\n",
    "        except:\n",
    "            index = -1\n",
    "        if index != -1:\n",
    "            tot += (i + 1 + index) ** (-1)\n",
    "    denominatore = 1.0\n",
    "    for i in range(1, overlap + 1):\n",
    "        denominatore += (2 * i) ** (-1)\n",
    "    return tot / denominatore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "def compute_score_sentence(sentence, synsets_context, total_weight):\n",
    "    synsets_sentence = []\n",
    "    score_sentence = 0\n",
    "    for word in sentence:\n",
    "        synsets_sentence.append(simplified_lesk(get_babelnet_synset_by_word(word), set([tupla[0] for tupla in synsets_context])))\n",
    "    for syn_word in synsets_sentence:\n",
    "        score_word = 0\n",
    "        for (syn_topic,weight) in synsets_context:\n",
    "            score_word += (weighted_overlap2(syn_word, syn_topic)*weight)\n",
    "        score_word /= total_weight #media ponderata\n",
    "        score_sentence += score_word\n",
    "    score_sentence /= len(sentence)\n",
    "    return score_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def disambiguate_word_context(weighted_context):\n",
    "    synsets_topic = []\n",
    "    total_weight = 0\n",
    "    for (word_topic, weight) in weighted_context:\n",
    "        synset_word_context = simplified_lesk(get_babelnet_synset_by_word(word_topic), set([tupla[0] for tupla in weighted_context if tupla[0] != word_topic]))#disambiguo il termine del topic utilizzando lesk, in cui come contesto di disambiguazione utilizzo il contesto eccetto la parola corrente\n",
    "        print('il synset della parola del contesto:',word_topic, ' è -> ', synset_word_context)\n",
    "        if synset_word_context is not None:\n",
    "            synsets_topic.append((synset_word_context, weight))\n",
    "            total_weight += weight\n",
    "    return synsets_topic, total_weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_overlap(sentence, weighted_context):\n",
    "    numeratore = 0\n",
    "    for word in sentence:\n",
    "        #check if word is in first column of key_words\n",
    "        if word in [x[0] for x in weighted_context]:\n",
    "            #print(f'{word} in key_words')\n",
    "            #get index of word in key_words\n",
    "            index = [x[1] for x in weighted_context if x[0] == word][0]\n",
    "            #print(f'index: {index}')\n",
    "\n",
    "            numeratore += 1/(index)\n",
    "            #print('\\n')\n",
    "       # else:\n",
    "            #print(f'{word} not in key_words')\n",
    "            #print('\\n')\n",
    "\n",
    "    i = 1\n",
    "    denominatore = 0\n",
    "    for word in weighted_context:\n",
    "        denominatore += 1/(2*i)\n",
    "        i += 1\n",
    "\n",
    "    return numeratore/denominatore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione automatica del riassunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summarization(text, text_preprocessed, synsets_context, total_weight, perc=0.8):\n",
    "    text_preprocessed = text_preprocessed[1:]\n",
    "\n",
    "    title = text[0]\n",
    "    text = text[1:] #rimuovo il titolo\n",
    "\n",
    "    weight_sentences = []\n",
    "    i = 0\n",
    "    print('combaciano: ', len(text) == len(text_preprocessed))\n",
    "    for line in text_preprocessed:\n",
    "        # attribuisce un peso ad ogni frase\n",
    "        weight_sentences.append((i, text[i], compute_score_sentence(line, synsets_context, total_weight)))\n",
    "        i += 1\n",
    "\n",
    "    # ordina le frasi in base al peso\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[2], reverse=True)\n",
    "    # prendi il primo 80% delle weight_sentences\n",
    "    weight_sentences = weight_sentences[:round(len(weight_sentences) * perc)]\n",
    "    # ordina le frasi in base all'id\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[0])\n",
    "\n",
    "    # prendi solo le frasi\n",
    "    summary = [x[1] for x in weight_sentences]\n",
    "    #add in the first postizion the title\n",
    "    summary.insert(0, title)\n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    summary = '\\n\\n'.join(summary)\n",
    "    print(summary)\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Andy-Warhol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'bn:14817595n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14817443n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01145355n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00495446n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00004020n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:27153738r', 'pos': 'ADV', 'source': 'BABELNET'}, {'id': 'bn:00212143n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00098021a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:00103780a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:00098352a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:03250251n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:22603416n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:15992024n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14239337n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03815618n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01578137n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:06189628n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00099226a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:00103781a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:15600118n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00660443n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00041551n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00103779a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:19774228n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:15826008n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00072620n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01502435n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091847v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:25331891n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:02455074n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091843v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:08648539n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:24197427n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03148087n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:23484542n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00116650r', 'pos': 'ADV', 'source': 'BABELNET'}, {'id': 'bn:00024989n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00065722n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00063555n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00227385n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14083195n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00433409n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:26363429n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:07215566n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01322283n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091846v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:06347784n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01975934n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091842v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:02085350n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03766496n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14043051n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:02927539n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00063586n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01875683n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00009616n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:15972592n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01421396n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:15882410n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091849v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:03302637n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:21135698n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03159734n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091845v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:16981223n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:23651616n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:07363804n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00015881n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:17821735n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00935198n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091841v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:00086062v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:22978554n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00084194v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:25755764n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:02843261n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03791418n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:08927114n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:24191281n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00063557n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14425496n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03832050n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01617919n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:08472068n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091848v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:08738015n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091844v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:02100952n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:24279685n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00903128n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00091840v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:01587787n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00108803a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:27268592n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00045726n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00457128n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00063556n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:08055581n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00083396v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:01307601n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:08021781n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14309814n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:13806035n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00852101n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:17717959n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03025354n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:02525546n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14581619n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:21547192n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:09754526n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:18322392n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00006182n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:15115221n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:25171848n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00060201n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:23997700n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:27110869a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:23912237n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:24434725n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:25097239n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00077016n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:25097229n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:22222067n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00017339n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00001963n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00059168n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:06784326n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:02286194n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14820378n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:07621740n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:22239344n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00045800n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:00095194v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:03259764n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00048180n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:14402740n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:06537684n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00095195v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:21138284n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:01915265n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00022750n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00078468n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:21728610n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:21759084n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00093157v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:00078467n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:21130283n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00083441v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:00350266n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:16789269n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:03266978n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:03445090n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:27600735n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00116064r', 'pos': 'ADV', 'source': 'BABELNET'}]\n",
      "[{'id': 'bn:02213915n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00099012a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:00098825a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:00503376n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:00463077n', 'pos': 'NOUN', 'source': 'BABELNET'}, {'id': 'bn:13709554v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:13709553v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:00099558a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:00099557a', 'pos': 'ADJ', 'source': 'BABELNET'}, {'id': 'bn:13709552v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:13709551v', 'pos': 'VERB', 'source': 'BABELNET'}, {'id': 'bn:13760480r', 'pos': 'ADV', 'source': 'BABELNET'}]\n"
     ]
    }
   ],
   "source": [
    "title_preprocessed, text_preprocessed, text, context_for_wsd = open_text(file_name)\n",
    "bn_ids = get_sentence_babelnet_ids(file_name, title_preprocessed)\n",
    "title_sense = get_best_senses(bn_ids, context_for_wsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_nasari_vectors = get_nasari_vectors_by_senses(title_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_context = get_weighted_context(title_nasari_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Your key is not valid or the daily requests limit has been reached. Please visit http://babelnet.org.'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2980\\2790413097.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0msynsets_context\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_weight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdisambiguate_word_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mweighted_context\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mweighted_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2980\\3286270220.py\u001B[0m in \u001B[0;36mdisambiguate_word_context\u001B[1;34m(weighted_context)\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mtotal_weight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mword_topic\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mweighted_context\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m         \u001B[0msynset_word_context\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msimplified_lesk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_babelnet_synset_by_word\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_topic\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtupla\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtupla\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mweighted_context\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mtupla\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mword_topic\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;31m#disambiguo il termine del topic utilizzando lesk, in cui come contesto di disambiguazione utilizzo il contesto eccetto la parola corrente\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'il synset della parola del contesto:'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mword_topic\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m' è -> '\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msynset_word_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0msynset_word_context\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2980\\2978368563.py\u001B[0m in \u001B[0;36msimplified_lesk\u001B[1;34m(bn_syns, context)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mbn_syns\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbn_syns\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbn_syns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m         \u001B[0mbest_sense\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbn_syns\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'id'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m         \u001B[0mmax_overlap\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "synsets_context, total_weight = disambiguate_word_context(weighted_context=weighted_context)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = make_summarization(text, text_preprocessed, synsets_context, total_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione\n",
    "\n",
    "la valutazione può essere eseguita sulla base di due metriche complementari\n",
    "- BLEU (bilingual evaluation understudy) per quanto riguarda la precision\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) per quanto riguarda la recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "funzione di scoring che è stata elaborata per valutare i sistemi per la traduzione automatica\n",
    "- costruire un sommario di riferimento, come un elenco di termini rilevanti che dovrebbero essere presenti.\n",
    "- confrontare l'insieme di termini nel riepilogo automatico (che chiamiamo riepilogo del candidato) con quelli nel riepilogo del candidato.\n",
    "- il punteggio BLEU è calcolato come P = m/wt che è la frazione di termini del candidato che si trovano nel riferimento, dove m è il numero di termini del candidato che sono nel riferimento, e wt è la dimensione di il candidato\n",
    "\n",
    "La precision in IR è solitamente definita come \n",
    "precision = |{relevant documents} ∩ {retrieved documents}| / |{retrieved documents}|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Precision: {count/len(automatic_sum)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECALL\n",
    "\n",
    "Questa metrica stima in che misura le parole (e/o n-grammi) nei riassunti di riferimento umano sono apparse nei riassunti creati dal sistema\n",
    "- ROUGE-N: Sovrapposizione di N-grammi tra candidato e riferimento\n",
    "riepilogo.\n",
    "- ROUGE-1 si riferisce alla sovrapposizione di unigramma (ogni parola) tra il sommari di sistema e di riferimento.\n",
    "\n",
    "La Recall in IR è abitualmente definito come recall =|{relevant documents} ∩ {retrieved documents}| / |{relevant documents}|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Recall: {count/len(manual_sum)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
