{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 5 - Automatic summarization\n",
    "\n",
    "Si vuole ridurre un documento del 10%, 20% o 30% secondo la seguente strategia\n",
    "\n",
    "1. Individuare l'argomento del testo che si sta riassumendo; l'argomento può essere indicato come un (insieme di) vettori NASARI:\n",
    "    - vt1 = {term1_score, term2_score, …, term10_score }\n",
    "    - vt2 = {term1_score, term2_score, …, term10_score } \n",
    "    - ...\n",
    "\n",
    "2. creare il contesto, raccogliendo qui i vettori dei termini (questo passaggio può essere ripetuto, scaricando ad ogni round il contributo dei termini associati)\n",
    "\n",
    "3. conservare i paragrafi le cui frasi contengono i termini più salienti, in base alla sovrapposizione ponderata, WO(v1,v2)\n",
    "    - riclassificare il peso dei paragrafi applicando almeno uno degli approcci menzionati (titolo, spunto, frase, coesione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "#BABELNET_TOKEN = '1e258739-f5e4-4961-8267-a2da4fe94572' #MO\n",
    "BABELNET_TOKEN = '01a5d861-2f36-45cb-8974-a2a6526530d2' #LT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Metodo utilizzato per eseguire il preprocessing delle frasi, in cui vengono effettuate le seguenti operazioni:\n",
    "- Rimozione della punteggiatura\n",
    "- Trasformazione delle lettere in lowercase\n",
    "- Tokenizzazione della frase tenendo conte delle multiword expression\n",
    "- Lemmatizzazione di tutte le parole\n",
    "- Rimozione delle stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #remove stop words\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_processing(document):\n",
    "    document = re.sub(r'[^\\w\\s]',' ',document) #remove punctuation\n",
    "    document = document.lower()\n",
    "    document = tokenizer.tokenize(document.split())\n",
    "    document = [lemmatizer.lemmatize(token) for token in document]  \n",
    "    document = [w for w in document if not w in stop_words]\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babelnet Id di una frase\n",
    "\n",
    "Viene utilizzato principalmente per ottenere i Babelnet Id delle parole del titolo, che una volta sottosposti a WSD (in quanto, molto probabilmente, per ogni parola avremo più synset) ci serviranno per ottenere i vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data una frase restituisce tutti i suoi babelnet id\n",
    "'''\n",
    "def get_sentence_babelnet_ids(file_name, sentence):\n",
    "    if os.path.exists('data/ids-'+ file_name +'.json'):\n",
    "        with open('data/ids-'+ file_name +'.json') as json_file:\n",
    "            ids = json.load(json_file)\n",
    "    else:\n",
    "        ids = {}\n",
    "        # prendo gli id di babelnet per ogni parola della frase\n",
    "        for word in sentence:\n",
    "            ids[word] = requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "        # prendo i synset di babelnet per ogni parola della frase\n",
    "        with open('data/ids_'+ file_name +'.json', 'w') as fp:\n",
    "            json.dump(ids, fp)\n",
    "\n",
    "    return ids\n",
    "\n",
    "'''\n",
    "Dato un babelnet id guardo nel file locale se ho già le informazioni, altrimenti \n",
    "faccio una richiesta a babelnet e aggiungo la riga al file locale\n",
    "'''\n",
    "def get_babelnet_synset_by_id(syn_id):\n",
    "    df = pd.read_csv('data/local_babelnet_syns.csv')\n",
    "    glosses = ['']\n",
    "    examples = ['']\n",
    "\n",
    "    if syn_id in df['id'].values:\n",
    "        row = df[df['id'] == syn_id]\n",
    "        name = row['name'].values[0]\n",
    "        if not 'nan' in str(row['glosses'].values[0]):\n",
    "            glosses = row['glosses'].values[0].split(';')\n",
    "        if not 'nan' in str(row['examples'].values[0]):\n",
    "            examples = row['examples'].values[0].split(';')             \n",
    "    else:\n",
    "        response = requests.get(f'https://babelnet.io/v8/getSynset?id={syn_id}&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "\n",
    "        print('=============')\n",
    "        print(response)\n",
    "        print('=============')\n",
    "\n",
    "        name = response['senses'][0]['properties']['fullLemma']\n",
    "        glosses = \"\"\n",
    "        for gloss in response['glosses']:\n",
    "            glosses += str(gloss['gloss']) + ';'\n",
    "        examples = \"\"\n",
    "        for example in response['examples']:\n",
    "            examples += str(example['example']) + ';'\n",
    "\n",
    "        #add row to df and save it\n",
    "        df = df.concat({'id': syn_id, 'name': name, 'glosses': glosses, 'examples': examples}, ignore_index=True)\n",
    "        df.to_csv('data/local_babelnet_syns.csv', index=False)\n",
    "\n",
    "        glosses = glosses.split(';')\n",
    "        examples = examples.split(';')\n",
    "\n",
    "    return syn_id, name, glosses, examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasari\n",
    "\n",
    "Estrazione dei vettori Nasari dal file locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors():\n",
    "    nasari_vectors = pd.read_csv('data/dd-nasari.txt', on_bad_lines='skip', header=None, sep=';')\n",
    "    nasari_vectors = nasari_vectors.set_index(0)\n",
    "    nasari_vectors[1].fillna('', inplace=True)    \n",
    "\n",
    "    return nasari_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Simplified Lesk per disambiguare il titolo\n",
    "\n",
    "Mi server per fare il WSD dei synsets ottenuti dei token del titolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dati i tutti i synset di una parola e il suo contesto, andando ad applicare\n",
    "il Lesk, restituisce il synset con il contesto più simile\n",
    "'''\n",
    "def get_signature(bn_syn):\n",
    "    _, _, glosses, examples = get_babelnet_synset_by_id(bn_syn)\n",
    "\n",
    "    signature = \"\"\n",
    "    for gloss in glosses:\n",
    "        signature += gloss + ' '\n",
    "    for example in examples:\n",
    "        signature += example + ' '\n",
    "    return set(pre_processing(signature))\n",
    "\n",
    "# Usa come contesto l'intero testo del file, non solo il titolo\n",
    "def simplified_lesk_by_syns(bn_syns, context):\n",
    "    best_sense = None\n",
    "    if bn_syns is not None and len(bn_syns) > 0:\n",
    "        best_sense = bn_syns[0]['id']\n",
    "        max_overlap = 0\n",
    "\n",
    "        for bn_syn in bn_syns:\n",
    "            signature = get_signature(bn_syn['id'])\n",
    "            overlap = len(context.intersection(signature))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = bn_syn['id']\n",
    "    return best_sense\n",
    "\n",
    "def get_best_senses(ids, context):\n",
    "    senses = {}\n",
    "    for word in ids:\n",
    "        senses[word] = simplified_lesk_by_syns(ids[word], context)\n",
    "\n",
    "    return senses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del documento da riassumere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(file_name):\n",
    "    text = open('data/docs/' + file_name + '.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    # pre processing\n",
    "    text = [line for line in text if line != '']\n",
    "    text_preprocessed = [pre_processing(line) for line in text[0:]]\n",
    "\n",
    "    # prendo il contesto per fare WSD dei babelnet id delle parole del testo\n",
    "    context_for_wsd = []\n",
    "    for sentence in text_preprocessed:\n",
    "        context_for_wsd = context_for_wsd + sentence\n",
    "    context_for_wsd = set(context_for_wsd)\n",
    "\n",
    "    # prendo il titolo\n",
    "    title_preprocessed = text_preprocessed[0]\n",
    "\n",
    "    return title_preprocessed, text_preprocessed, text, context_for_wsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associazione Nasari Vectors - Babelnet Synsets\n",
    "\n",
    "Associazione dei vettori Nasari ai Synset disambiguati delle parole del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Per ogni senso presente nel dizionario senses, restituisce il vettore nasari corrispondente se esiste\n",
    "'''\n",
    "def get_nasari_vectors_by_senses(senses, nasari_vectors):\n",
    "    vectors = {}\n",
    "\n",
    "    for sense in senses:\n",
    "        v = get_nasari_vectors_by_sense(senses[sense], nasari_vectors)\n",
    "        if v is not None:\n",
    "            vectors[sense] = v\n",
    "\n",
    "    return vectors\n",
    "\n",
    "'''\n",
    "Dato un senso (cioè un babelnet id), restituisce il vettore nasari corrispondente se esiste\n",
    "'''\n",
    "def get_nasari_vectors_by_sense(sense, nasari_vectors):\n",
    "    if sense in list(nasari_vectors.index.values):\n",
    "        return nasari_vectors.loc[sense]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_topic(vectors):\n",
    "    get_weighted_context = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        #print(vectors[vector])\n",
    "        sum_weights = 0\n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:\n",
    "                sum_weights += float(word.split('_')[1])\n",
    "            \n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:    \n",
    "                weight = float(word.split('_')[1]) / sum_weights\n",
    "                # preprocess word\n",
    "                word_to_added = word.split('_')[0]\n",
    "                word_to_added = re.sub(r'[^\\w\\s]',' ',word_to_added) #remove punctuation\n",
    "                word_to_added = word_to_added.lower()\n",
    "                word_to_added = lemmatizer.lemmatize(word_to_added)\n",
    "\n",
    "                get_weighted_context.append((word_to_added, weight))\n",
    "\n",
    "    return get_weighted_context\n",
    "\n",
    "def get_nasari_vector_for_topic(title_nasari_vectors, weighted_topic, nasari_vectors):\n",
    "    context = set([x[0] for x in weighted_topic])\n",
    "\n",
    "    weighted_topic_vectors = []\n",
    "    for token, weight in weighted_topic:\n",
    "        if token in title_nasari_vectors:\n",
    "            weighted_topic_vectors.append((token, weight))\n",
    "        else:\n",
    "            best_sense = simplified_lesk_for_paragraphs_word(token, context, nasari_vectors)\n",
    "            if best_sense is not None:\n",
    "                weighted_topic_vectors.append((best_sense, weight))\n",
    "\n",
    "    return weighted_topic_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribuzione score ai paragrafi\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ottenimento dei vettori nasari delle parole nel paragrafo\n",
    "\n",
    "Data una parola x della frase, andiamo a cercare in nasari_vectors tutti i vettori nasari che hanno x come testa. Fatto ciò, probabilmente, avremo più vettori per ogni parola (quindi più sensi per la parola), occorre quindi fare WSD su di essi. Per fare ciò ricorriamo all'utilizzo di un'altra implementazione del simplified lesk che andrà ad usare come contesto le parole presenti nel topic, mentre come signature le parole presenti nel vettore nasari del senso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Restituisce i vettori nasari che contengono nella testa la parola word\n",
    "'''\n",
    "def get_nasari_vectors_by_token(token, nasari_vectors):\n",
    "    return nasari_vectors[nasari_vectors[1].str.contains(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Per ogni vettore nasari del token, restituisce un dizionario con chiave il babelnet id e valore le parole\n",
    "contenute nel vettore nasari\n",
    "'''\n",
    "def get_context_foreach_token_sense(nasari_vectors_by_token):\n",
    "    context = {}\n",
    "    for index, row in nasari_vectors_by_token.iterrows():\n",
    "        bn_id = row.name\n",
    "        row_values = row.iloc[1:].apply(lambda x: str(x).split('_')[0] if len(str(x).split('_')) > 0 else '').values\n",
    "        concatenated_row = ' '.join(row_values)\n",
    "        context[bn_id] = set(pre_processing(concatenated_row))\n",
    "    \n",
    "    if context == {}:\n",
    "        return None\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts_of_sentence_word_from_nasari(word, nasari_vectors):\n",
    "    nasari_vectors_by_token = get_nasari_vectors_by_token(word, nasari_vectors)\n",
    "    if nasari_vectors_by_token.empty:\n",
    "        return None\n",
    "    else:\n",
    "        return get_context_foreach_token_sense(nasari_vectors_by_token)\n",
    "\n",
    "def simplified_lesk_for_paragraphs_word(word, context, nasari_vectors):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    word_signature = get_contexts_of_sentence_word_from_nasari(word, nasari_vectors)\n",
    "    if word_signature is not None and len(word_signature) > 0:\n",
    "        for bn_syn in word_signature.keys():\n",
    "            signature = word_signature[bn_syn]\n",
    "            overlap = len(context.intersection(signature))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = bn_syn\n",
    "\n",
    "    return best_sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesatura dei paragrafi\n",
    "\n",
    "Eseguita usando la metrica Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weighted overlap\n",
    "def weighted_overlap2(vect1, vect2):\n",
    "    tot = 0.0\n",
    "    overlap = 0\n",
    "    for i, elem in enumerate(vect1):\n",
    "        try:\n",
    "            index = vect2.index(elem) + 1\n",
    "            overlap += 1\n",
    "        except:\n",
    "            index = -1\n",
    "        if index != -1:\n",
    "            tot += (i + 1 + index) ** (-1)\n",
    "    denominatore = 1.0\n",
    "    for i in range(1, overlap + 1):\n",
    "        denominatore += (2 * i) ** (-1)\n",
    "        \n",
    "    return tot / denominatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_score_paragraph(paragraph, synsets_context, total_weight, nasari_vectors):\n",
    "    synsets_sentence = []\n",
    "    score_sentence = 0\n",
    "\n",
    "    i = 0\n",
    "    for word in paragraph:\n",
    "        best_sense = simplified_lesk_for_paragraphs_word(word, set([tupla[0] for tupla in synsets_context]), nasari_vectors)\n",
    "        if best_sense is not None:\n",
    "            synsets_sentence.append(best_sense)\n",
    "        print(f'[{i}/{len(paragraph)}] {word} -> {synsets_sentence[i]}')\n",
    "        i += 1\n",
    "\n",
    "    for syn_word in synsets_sentence:\n",
    "        score_word = 0\n",
    "        print(f'[{i}/{len(paragraph)}] {syn_word}')\n",
    "        for (syn_topic,weight) in synsets_context:\n",
    "            score_word += (weighted_overlap2(syn_word, syn_topic)*weight)\n",
    "            print(f'[{i}/{len(paragraph)}] {syn_word} -> {syn_topic} -> {weighted_overlap2(syn_word, syn_topic)*weight}')\n",
    "        score_word /= total_weight #media ponderata\n",
    "        score_sentence += score_word\n",
    "    score_sentence /= len(paragraph)\n",
    "    return score_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_weight(weighted_topic_vectors):\n",
    "    total_weight = 0\n",
    "    for _, weight in weighted_topic_vectors:\n",
    "            total_weight += weight\n",
    "\n",
    "    return total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def disambiguate_word_context(weighted_context):\n",
    "    synsets_topic = []\n",
    "    total_weight = 0\n",
    "    for (word_topic, weight) in weighted_context:\n",
    "        synset_word_context = simplified_lesk(get_babelnet_synset_by_word(word_topic), set([tupla[0] for tupla in weighted_context if tupla[0] != word_topic]))#disambiguo il termine del topic utilizzando lesk, in cui come contesto di disambiguazione utilizzo il contesto eccetto la parola corrente\n",
    "        print('il synset della parola del contesto:',word_topic, ' è -> ', synset_word_context)\n",
    "        if synset_word_context is not None:\n",
    "            synsets_topic.append((synset_word_context, weight))\n",
    "            total_weight += weight\n",
    "    return synsets_topic, total_weight'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def disambiguate_word_context(weighted_context):\n",
    "    synsets_topic = []\n",
    "    total_weight = 0\n",
    "    for (word_topic, weight) in weighted_context:\n",
    "        synset_word_context = simplified_lesk(get_babelnet_synset_by_word(word_topic), set([tupla[0] for tupla in weighted_context if tupla[0] != word_topic]))#disambiguo il termine del topic utilizzando lesk, in cui come contesto di disambiguazione utilizzo il contesto eccetto la parola corrente\n",
    "        print('il synset della parola del contesto:',word_topic, ' è -> ', synset_word_context)\n",
    "        if synset_word_context is not None:\n",
    "            synsets_topic.append((synset_word_context, weight))\n",
    "            total_weight += weight\n",
    "    return synsets_topic, total_weight'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def weighted_overlap(sentence, weighted_context):\\n    numeratore = 0\\n    for word in sentence:\\n        #check if word is in first column of key_words\\n        if word in [x[0] for x in weighted_context]:\\n            #print(f'{word} in key_words')\\n            #get index of word in key_words\\n            index = [x[1] for x in weighted_context if x[0] == word][0]\\n            #print(f'index: {index}')\\n\\n            numeratore += 1/(index)\\n            #print('\\n')\\n       # else:\\n            #print(f'{word} not in key_words')\\n            #print('\\n')\\n\\n    i = 1\\n    denominatore = 0\\n    for word in weighted_context:\\n        denominatore += 1/(2*i)\\n        i += 1\\n\\n    return numeratore/denominatore\""
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def weighted_overlap(sentence, weighted_context):\n",
    "    numeratore = 0\n",
    "    for word in sentence:\n",
    "        #check if word is in first column of key_words\n",
    "        if word in [x[0] for x in weighted_context]:\n",
    "            #print(f'{word} in key_words')\n",
    "            #get index of word in key_words\n",
    "            index = [x[1] for x in weighted_context if x[0] == word][0]\n",
    "            #print(f'index: {index}')\n",
    "\n",
    "            numeratore += 1/(index)\n",
    "            #print('\\n')\n",
    "       # else:\n",
    "            #print(f'{word} not in key_words')\n",
    "            #print('\\n')\n",
    "\n",
    "    i = 1\n",
    "    denominatore = 0\n",
    "    for word in weighted_context:\n",
    "        denominatore += 1/(2*i)\n",
    "        i += 1\n",
    "\n",
    "    return numeratore/denominatore'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione automatica del riassunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summarization(text, text_preprocessed, synsets_context, total_weight, nasari_vectors, perc=0.8):\n",
    "    text_preprocessed = text_preprocessed[1:]\n",
    "\n",
    "    title = text[0]\n",
    "    text = text[1:] #rimuovo il titolo\n",
    "\n",
    "    weight_sentences = []\n",
    "    i = 0\n",
    "\n",
    "    for paragraph in text_preprocessed:\n",
    "        # attribuisce un peso ad ogni frase\n",
    "        weight_sentences.append((i, text[i], compute_score_paragraph(paragraph, synsets_context, total_weight, nasari_vectors)))\n",
    "        print(f'peso frase {i}: {weight_sentences[i][2]}')\n",
    "        i += 1\n",
    "\n",
    "    # ordina le frasi in base al peso\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[2], reverse=True)\n",
    "    # prendi il primo 80% delle weight_sentences\n",
    "    weight_sentences = weight_sentences[:round(len(weight_sentences) * perc)]\n",
    "    # ordina le frasi in base all'id\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[0])\n",
    "\n",
    "    # prendi solo le frasi\n",
    "    summary = [x[1] for x in weight_sentences]\n",
    "    #add in the first postizion the title\n",
    "    summary.insert(0, title)\n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    summary = '\\n\\n'.join(summary)\n",
    "    print(summary)\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Andy-Warhol'\n",
    "nasari_vectors = get_nasari_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_preprocessed, text_preprocessed, text, context_for_wsd_title = open_text(file_name)\n",
    "\n",
    "# prendo il titolo e ottengo i sensi corretti dei token del titolo\n",
    "bn_ids = get_sentence_babelnet_ids(file_name, title_preprocessed)\n",
    "title_senses = get_best_senses(bn_ids, context_for_wsd_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottenimento del topic\n",
    "title_nasari_vectors = get_nasari_vectors_by_senses(title_senses, nasari_vectors)\n",
    "weighted_topic = get_weighted_topic(title_nasari_vectors)\n",
    "weighted_topic_vectors = get_nasari_vector_for_topic(title_nasari_vectors, weighted_topic, nasari_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight = get_total_weight(weighted_topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/27] anticipated -> None\n",
      "[1/27] celebrity -> None\n",
      "[2/27] culture -> bn:00211860n\n",
      "[3/27] social -> bn:01316426n\n",
      "[4/27] medium -> None\n",
      "[5/27] thought -> bn:00077029n\n",
      "[6/27] artist -> bn:01017615n\n",
      "[7/27] more than -> None\n",
      "[8/27] hold -> bn:00738264n\n",
      "[9/27] paintbrush -> None\n",
      "[10/27] wound up -> None\n",
      "[11/27] john lennon -> None\n",
      "[12/27] new -> bn:00775066n\n",
      "[13/27] tate -> bn:00691280n\n",
      "[14/27] exhibition -> bn:00005938n\n",
      "[15/27] open -> bn:02590911n\n",
      "[16/27] alastair -> None\n",
      "[17/27] smart -> None\n",
      "[18/27] show -> bn:00903557n\n",
      "[19/27] far -> bn:02004026n\n",
      "[20/27] important -> None\n",
      "[21/27] artist -> bn:01017615n\n",
      "[22/27] modern -> bn:00412461n\n",
      "[23/27] age -> bn:00020593n\n",
      "[24/27] wa -> bn:01938240n\n",
      "[25/27] ahead -> None\n",
      "[26/27] time -> bn:00506951n\n",
      "[27/27] None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[311], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summary \u001b[39m=\u001b[39m make_summarization(text, text_preprocessed, weighted_topic_vectors, total_weight, nasari_vectors)\n",
      "Cell \u001b[1;32mIn[305], line 12\u001b[0m, in \u001b[0;36mmake_summarization\u001b[1;34m(text, text_preprocessed, synsets_context, total_weight, nasari_vectors, perc)\u001b[0m\n\u001b[0;32m      8\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m paragraph \u001b[39min\u001b[39;00m text_preprocessed:\n\u001b[0;32m     11\u001b[0m     \u001b[39m# attribuisce un peso ad ogni frase\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     weight_sentences\u001b[39m.\u001b[39mappend((i, text[i], compute_score_paragraph(paragraph, synsets_context, total_weight, nasari_vectors)))\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpeso frase \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mweight_sentences[i][\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[309], line 15\u001b[0m, in \u001b[0;36mcompute_score_paragraph\u001b[1;34m(paragraph, synsets_context, total_weight, nasari_vectors)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(paragraph)\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00msyn_word\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m (syn_topic,weight) \u001b[39min\u001b[39;00m synsets_context:\n\u001b[1;32m---> 15\u001b[0m     score_word \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (weighted_overlap2(syn_word, syn_topic)\u001b[39m*\u001b[39mweight)\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(paragraph)\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00msyn_word\u001b[39m}\u001b[39;00m\u001b[39m -> \u001b[39m\u001b[39m{\u001b[39;00msyn_topic\u001b[39m}\u001b[39;00m\u001b[39m -> \u001b[39m\u001b[39m{\u001b[39;00mweighted_overlap2(syn_word, syn_topic)\u001b[39m*\u001b[39mweight\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m score_word \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m total_weight \u001b[39m#media ponderata\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[310], line 5\u001b[0m, in \u001b[0;36mweighted_overlap2\u001b[1;34m(vect1, vect2)\u001b[0m\n\u001b[0;32m      3\u001b[0m tot \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m      4\u001b[0m overlap \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m i, elem \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(vect1):\n\u001b[0;32m      6\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         index \u001b[39m=\u001b[39m vect2\u001b[39m.\u001b[39mindex(elem) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "summary = make_summarization(text, text_preprocessed, weighted_topic_vectors, total_weight, nasari_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione\n",
    "\n",
    "la valutazione può essere eseguita sulla base di due metriche complementari\n",
    "- BLEU (bilingual evaluation understudy) per quanto riguarda la precision\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) per quanto riguarda la recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "funzione di scoring che è stata elaborata per valutare i sistemi per la traduzione automatica\n",
    "- costruire un sommario di riferimento, come un elenco di termini rilevanti che dovrebbero essere presenti.\n",
    "- confrontare l'insieme di termini nel riepilogo automatico (che chiamiamo riepilogo del candidato) con quelli nel riepilogo del candidato.\n",
    "- il punteggio BLEU è calcolato come P = m/wt che è la frazione di termini del candidato che si trovano nel riferimento, dove m è il numero di termini del candidato che sono nel riferimento, e wt è la dimensione di il candidato\n",
    "\n",
    "La precision in IR è solitamente definita come \n",
    "precision = |{relevant documents} ∩ {retrieved documents}| / |{retrieved documents}|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Precision: {count/len(automatic_sum)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECALL\n",
    "\n",
    "Questa metrica stima in che misura le parole (e/o n-grammi) nei riassunti di riferimento umano sono apparse nei riassunti creati dal sistema\n",
    "- ROUGE-N: Sovrapposizione di N-grammi tra candidato e riferimento\n",
    "riepilogo.\n",
    "- ROUGE-1 si riferisce alla sovrapposizione di unigramma (ogni parola) tra il sommari di sistema e di riferimento.\n",
    "\n",
    "La Recall in IR è abitualmente definito come recall =|{relevant documents} ∩ {retrieved documents}| / |{relevant documents}|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Recall: {count/len(manual_sum)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
