{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic summarization\n",
    "\n",
    "Risorse usate:\n",
    "- Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "\n",
    "BABELNET_TOKEN = '1e258739-f5e4-4961-8267-a2da4fe94572' #MO\n",
    "#BABELNET_TOKEN = '01a5d861-2f36-45cb-8974-a2a6526530d2' #LT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Metodo utilizzato per eseguire il preprocessing delle frasi, in cui vengono effettuate le seguenti operazioni:\n",
    "- Rimozione della punteggiatura\n",
    "- Trasformazione delle lettere in lowercase\n",
    "- Tokenizzazione della frase tenendo conte delle multiword expression\n",
    "- Lemmatizzazione di tutte le parole\n",
    "- Rimozione delle stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #remove stop words\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_processing(document):\n",
    "    document = re.sub(r'[^\\w\\s]',' ',document) #remove punctuation\n",
    "    document = document.lower()\n",
    "    document = tokenizer.tokenize(document.split())\n",
    "    document = [lemmatizer.lemmatize(token) for token in document]  \n",
    "    document = [w for w in document if not w in stop_words]\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babelnet Id di una frase\n",
    "\n",
    "Viene utilizzato principalmente per ottenere i Babelnet Id delle parole del titolo, che una volta sottosposti a WSD (in quanto, molto probabilmente, per ogni parola avremo più synset) ci serviranno per ottenere i vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data una frase restituisce tutti i suoi bablenet id\n",
    "'''\n",
    "def get_sentence_babelnet_ids(file_name, sentence):\n",
    "    if os.path.exists('data/ids-'+ file_name +'.json'):\n",
    "        with open('data/ids-'+ file_name +'.json') as json_file:\n",
    "            ids = json.load(json_file)\n",
    "    else:\n",
    "        ids = {}\n",
    "        # prendo gli id di babelnet per ogni parola della frase\n",
    "        for word in sentence:\n",
    "            ids[word] = requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "        # prendo i synset di babelnet per ogni parola della frase\n",
    "        with open('data/ids_'+ file_name +'.json', 'w') as fp:\n",
    "            json.dump(ids, fp)\n",
    "\n",
    "    return ids\n",
    "\n",
    "'''\n",
    "Dato un babelnet id guardo nel file locale se ho già le informazioni, altrimenti \n",
    "faccio una richiesta a babelnet e aggiungo la riga al file locale\n",
    "'''\n",
    "def get_babelnet_synset_by_id(syn_id):\n",
    "    df = pd.read_csv('data/local_babelnet_syns.csv')\n",
    "    name = \"\"\n",
    "    glosses = ['']\n",
    "    examples = ['']\n",
    "\n",
    "    if syn_id in df['id'].values:\n",
    "        row = df[df['id'] == syn_id]\n",
    "        name = row['name'].values[0]\n",
    "        if not 'nan' in str(row['glosses'].values[0]):\n",
    "            glosses = row['glosses'].values[0].split(';')\n",
    "        if not 'nan' in str(row['examples'].values[0]):\n",
    "            examples = row['examples'].values[0].split(';')             \n",
    "    else:\n",
    "        response = requests.get(f'https://babelnet.io/v8/getSynset?id={syn_id}&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "\n",
    "        print('=============')\n",
    "        print(response)\n",
    "        print('=============')\n",
    "\n",
    "        name = response['senses'][0]['properties']['fullLemma']\n",
    "        glosses = \"\"\n",
    "        for gloss in response['glosses']:\n",
    "            glosses += str(gloss['gloss']) + ';'\n",
    "        examples = \"\"\n",
    "        for example in response['examples']:\n",
    "            examples += str(example['example']) + ';'\n",
    "\n",
    "        #add row to df and save it\n",
    "        df = df.append({'id': syn_id, 'name': name, 'glosses': glosses, 'examples': examples}, ignore_index=True)\n",
    "        df.to_csv('data/local_babelnet_syns.csv', index=False)\n",
    "\n",
    "        glosses = glosses.split(';')\n",
    "        examples = examples.split(';')\n",
    "\n",
    "    return syn_id, name, glosses, examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasari\n",
    "\n",
    "Estrazione dei vettori Nasari dal file locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors():\n",
    "    nasari_vectors = pd.read_csv('data/dd-nasari.txt', on_bad_lines='skip', header=None, sep=';')\n",
    "    nasari_vectors = nasari_vectors.set_index(0)\n",
    "    return nasari_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Simplified Lesk\n",
    "\n",
    "Mi server per fare il WSD dei synsets ottenuti dei token del titolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dati i tutti i synset di una parola e il suo contesto, andando ad applicare\n",
    "il Lesk, restituisce il synset con il contesto più simile\n",
    "'''\n",
    "def get_signature(bn_syn):\n",
    "    _, _, glosses, examples = get_babelnet_synset_by_id(bn_syn)\n",
    "\n",
    "    signature = \"\"\n",
    "    for gloss in glosses:\n",
    "        signature += gloss + ' '\n",
    "    for example in examples:\n",
    "        signature += example + ' '\n",
    "    return set(pre_processing(signature))\n",
    "\n",
    "# Usa come contesto l'intero testo del file, non solo il titolo\n",
    "def simplified_lesk(bn_syns, context):\n",
    "    best_sense = bn_syns[0]['id']\n",
    "    max_overlap = 0\n",
    "    \n",
    "    for bn_syn in bn_syns:\n",
    "        signature = get_signature(bn_syn['id'])\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = bn_syn['id']\n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "def get_best_senses(ids, context):\n",
    "    senses = {}\n",
    "    for word in ids:\n",
    "        senses[word] = simplified_lesk(ids[word], context)\n",
    "\n",
    "    return senses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del documento da tradurre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(file_name):\n",
    "    text = open('data/docs/' + file_name + '.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    # pre processing\n",
    "    text = [line for line in text if line != '']\n",
    "    text_preprocessed = [pre_processing(line) for line in text[0:]]\n",
    "\n",
    "    # prendo il contesto per fare WSD dei babelnet id delle parole del testo\n",
    "    context_for_wsd = []\n",
    "    for sentence in text_preprocessed:\n",
    "        context_for_wsd = context_for_wsd + sentence\n",
    "    context_for_wsd = set(context_for_wsd)\n",
    "\n",
    "    # prendo il titolo\n",
    "    title_preprocessed = text_preprocessed[0]\n",
    "\n",
    "    return title_preprocessed, text_preprocessed, text, context_for_wsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associazione Vectors - Synsets\n",
    "\n",
    "Associazione dei vettori Nasari ai Synset disambiguati delle parole del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors_by_senses(senses):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    vectors = {}\n",
    "\n",
    "    for word in senses:\n",
    "        if senses[word] in list(nasari_vectors.index.values):\n",
    "            #print(f'{senses[word]} in nasari_vectors') \n",
    "            vectors[word] = nasari_vectors.loc[senses[word]]\n",
    "        #else:\n",
    "            #print(f'{senses[word]} not in nasari_vectors')\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del contesto per fare text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_context(vectors):\n",
    "    get_weighted_context = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        #print(vectors[vector])\n",
    "        sum_weights = 0\n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:\n",
    "                sum_weights += float(word.split('_')[1])\n",
    "            \n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:    \n",
    "                weight = float(word.split('_')[1]) / sum_weights\n",
    "                # preprocess word\n",
    "                word_to_added = word.split('_')[0]\n",
    "                word_to_added = re.sub(r'[^\\w\\s]',' ',word_to_added) #remove punctuation\n",
    "                word_to_added = word_to_added.lower()\n",
    "                word_to_added = lemmatizer.lemmatize(word_to_added)\n",
    "\n",
    "                get_weighted_context.append((word_to_added, weight))\n",
    "\n",
    "    return get_weighted_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesatura dei paragrafi\n",
    "\n",
    "Eseguita usanto la metrica Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_overlap(sentence, weighted_context):\n",
    "    numeratore = 0\n",
    "    for word in sentence:\n",
    "        #check if word is in first column of key_words\n",
    "        if word in [x[0] for x in weighted_context]:\n",
    "            #print(f'{word} in key_words')\n",
    "            #get index of word in key_words\n",
    "            index = [x[1] for x in weighted_context if x[0] == word][0]\n",
    "            #print(f'index: {index}')\n",
    "\n",
    "            numeratore += 1/(index)\n",
    "            #print('\\n')\n",
    "       # else:\n",
    "            #print(f'{word} not in key_words')\n",
    "            #print('\\n')\n",
    "\n",
    "    i = 1\n",
    "    denominatore = 0\n",
    "    for word in weighted_context:\n",
    "        denominatore += 1/(2*i)\n",
    "        i += 1\n",
    "\n",
    "    return numeratore/denominatore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione automatica del riassunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summarization(text, text_preprocessed, weighted_context, perc=0.8):\n",
    "    text_preprocessed = text_preprocessed[1:]\n",
    "\n",
    "    title = text[0]\n",
    "    text = text[1:]\n",
    "\n",
    "    weight_sentences = []\n",
    "    i = 0\n",
    "    for line in text_preprocessed:\n",
    "        # attribuisce un peso ad ogni frase\n",
    "        weight_sentences.append((i, text[i], weighted_overlap(line, weighted_context)))\n",
    "        i += 1\n",
    "\n",
    "    # ordina le frasi in base al peso\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[2], reverse=True)\n",
    "    # prendi il primo 80% delle weight_sentences\n",
    "    weight_sentences = weight_sentences[:round(len(weight_sentences) * perc)]\n",
    "    # ordina le frasi in base all'id\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[0])\n",
    "\n",
    "    # prendi solo le frasi\n",
    "    summary = [x[1] for x in weight_sentences]\n",
    "    summary = '\\n\\n'.join(summary)\n",
    "    summary = title + '\\n\\n' + summary\n",
    "    \n",
    "    print(summary)\n",
    "    return summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Andy-Warhol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_preprocessed, text_preprocessed, text, context_for_wsd = open_text(file_name)\n",
    "bn_ids = get_sentence_babelnet_ids(file_name, title_preprocessed)\n",
    "title_sense = get_best_senses(bn_ids, context_for_wsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_nasari_vectors = get_nasari_vectors_by_senses(title_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_context = get_weighted_context(title_nasari_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Warhol: Why the great Pop artist thought ‘Trump is sort of cheap’\n",
      "\n",
      "He anticipated celebrity culture and social media, thought artists should do more than just hold a paintbrush, and wound up John Lennon. As a new Tate exhibition opens, Alastair Smart shows how far the most important artist of the modern age was ahead of his time.\n",
      "\n",
      "uring last year’s Super Bowl, 100 million US viewers were treated to a most unexpected sight in one of the commercial breaks. It was Andy Warhol doing nothing more than taking bites out of a Burger King Whopper – and adding the occasional bit of ketchup – for 45 seconds.\n",
      "\n",
      "There was no music, no punchline, just a little, light rustling of the burger’s wrapper – in a slowly unfolding scene that culminated with the hashtag #EatLikeAndy. It was about as far removed as one could imagine from the big-budget ads traditionally shown during the Super Bowl.\n",
      "\n",
      "Reaction on social media was swift, widespread and mostly containing the word “bizarre”. Why show a low-action clip from 1982, of an artist who died in 1987, at a sporting showcase in 2019? What financial sense did it make also – given that advertising slots at the Super Bowl are the most expensive in the world, costing $175,000 a second?\n",
      "\n",
      "The answer is that the footage showed a great American artist eating a great American food in the context of a great American occasion. Burger King said the advert was a celebration of the country itself.\n",
      "\n",
      "After a decade working as a commercial illustrator on Madison Avenue, Warhol began his artistic career at the turn of the 1960s. He sprung to fame as a leader of the Pop Art movement, which rejected high-cultural tradition and depicted everyday subject matter instead: in Warhol’s case, Coke bottles, Brillo boxes and Campbell’s Soup cans.\n",
      "\n",
      "Soon he was silk-screening images of Marilyn Monroe and Elvis Presley too. It suggested an artist entirely at one with America’s average Joe. The irony is, of course, that under the current president’s administration, Warhol’s parents probably wouldn’t have been allowed into the United States in the first place. Andrej and Julia Warhola hailed from a tiny village called Mikova on the edge of the Carpathian Mountains, in what today is Slovakia (but in their day was part of the Austro-Hungarian empire). In search of a better life, they moved to Pittsburgh after the First World War, where Andrej worked in construction and where Andy – Andrew Warhola – was born in 1928.\n",
      "\n",
      "The Tate exhibition sets out to show how large an influence Warhol has had on artists after him, especially in the way he embraced different media and means of distribution. In the mid-1960s, after the early burst of Pop works, he took to making films, such as Sleep, in which his friend, the poet John Giorno, can be seen sleeping for five hours.\n",
      "\n",
      "He also founded Interview, a magazine in which celebrities from Cher to Michael Jackson were interviewed – and which ran for 50 years before its demise in 2018. And let’s not forget Exploding Plastic Inevitable, a multi-sensory stage show he conceived for the Velvet Underground, a band he briefly managed.\n",
      "\n",
      "In short, he believed an artist could, and should, do more than just hold a paintbrush. An interdisciplinary approach to art is today taken as standard – and that’s in no small part thanks to the example set by Warhol’s adventures decades ago.\n",
      "\n",
      "His influence extends far beyond the art world, though. “Warhol both predates and predicts the society we live in,” says Muir. On show at Tate Modern will be 25 paintings from a little-known series of paintings from 1975 called Ladies and Gentlemen. It depicts drag queens and trans women who frequented the Gilded Grape bar, off Times Square – and, in Muir’s view, “could easily pass for having been made yesterday, in terms of the debates being had now around trans identity and LGBT rights”.\n",
      "\n",
      "Though dead for 33 years, Warhol anticipated the advent of social media. Take his Screen Tests, for example, 472 short films in which visitors to his studio were placed on a chair in front of a video camera and asked to “perform” solo for three minutes to camera – very much like the YouTube vloggers of today.\n",
      "\n",
      "Then there’s his 1975 book The Philosophy of Andy Warhol, usually described as a disjointed autobiography but better understood as a proto-Twitter account, given its author’s propensity for including both the most mundane of details (such as his love of jam on toast) and witty one-liners (such as “buying is more American than thinking”).\n",
      "\n",
      "He also carried a compact Minox camera with him wherever he went, taking more than 100,000 photographs of friends, food, building facades, shop signs and the like – anticipating the Instagram feeds of our image-saturated age today. “I never read,” Warhol once said. “I just look at pictures.” (In his recently published autobiography, Me, Elton John tells the hilarious story of a time he and John Lennon had been taking piles of cocaine in a New York hotel room late into the night, when Warhol knocked at the door. As Elton went to answer, Lennon implored, “Don’t let him in: he’ll have his f***ing camera.”)\n",
      "\n",
      "He was aware of the inexorable rise of television, particularly with the proliferation of cable channels such as CNN, ESPN, Nickelodeon and MTV in the late-1970s and early-1980s. Warhol declared at the time that television “was the medium [he’d] most now like to shine in”, and he actually presented five episodes of an MTV talk show called Andy Warhol’s Fifteen Minutes – a job cut short by his death, aged 58, from complications after a gall bladder operation. The show’s title referred to his most famous quote: “In the future, everyone will be world-famous for 15 minutes.”\n",
      "\n",
      "“It’s often said Warhol was the most important artist of the second half of the 20th century,” Muir says. “But one might well argue he’s the most important artist of the early 21st century too.” In a consumerist society, he realised that celebrities are every bit as ubiquitous and disposable as soup cans, Coke bottles and Burger King Whoppers.\n"
     ]
    }
   ],
   "source": [
    "summary = make_summarization(text, text_preprocessed, weighted_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
