{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 5 - Automatic summarization\n",
    "\n",
    "Si vuole ridurre un documento del 10%, 20% o 30% secondo la seguente strategia\n",
    "\n",
    "1. Individuare l'argomento del testo che si sta riassumendo; l'argomento può essere indicato come un (insieme di) vettori NASARI:\n",
    "    - vt1 = {term1_score, term2_score, …, term10_score }\n",
    "    - vt2 = {term1_score, term2_score, …, term10_score } \n",
    "    - ...\n",
    "\n",
    "2. creare il contesto, raccogliendo qui i vettori dei termini (questo passaggio può essere ripetuto, scaricando ad ogni round il contributo dei termini associati)\n",
    "\n",
    "3. conservare i paragrafi le cui frasi contengono i termini più salienti, in base alla sovrapposizione ponderata, WO(v1,v2)\n",
    "    - riclassificare il peso dei paragrafi applicando almeno uno degli approcci menzionati (titolo, spunto, frase, coesione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "\n",
    "BABELNET_TOKEN = '1e258739-f5e4-4961-8267-a2da4fe94572' #MO\n",
    "#BABELNET_TOKEN = '01a5d861-2f36-45cb-8974-a2a6526530d2' #LT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Metodo utilizzato per eseguire il preprocessing delle frasi, in cui vengono effettuate le seguenti operazioni:\n",
    "- Rimozione della punteggiatura\n",
    "- Trasformazione delle lettere in lowercase\n",
    "- Tokenizzazione della frase tenendo conte delle multiword expression\n",
    "- Lemmatizzazione di tutte le parole\n",
    "- Rimozione delle stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #remove stop words\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_processing(document):\n",
    "    document = re.sub(r'[^\\w\\s]',' ',document) #remove punctuation\n",
    "    document = document.lower()\n",
    "    document = tokenizer.tokenize(document.split())\n",
    "    document = [lemmatizer.lemmatize(token) for token in document]  \n",
    "    document = [w for w in document if not w in stop_words]\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babelnet Id di una frase\n",
    "\n",
    "Viene utilizzato principalmente per ottenere i Babelnet Id delle parole del titolo, che una volta sottosposti a WSD (in quanto, molto probabilmente, per ogni parola avremo più synset) ci serviranno per ottenere i vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_babelnet_synset_by_word(word):\n",
    "    return requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "'''\n",
    "Data una frase restituisce tutti i suoi babelnet id\n",
    "'''\n",
    "def get_sentence_babelnet_ids(file_name, sentence):\n",
    "    if os.path.exists('data/ids-'+ file_name +'.json'):\n",
    "        with open('data/ids-'+ file_name +'.json') as json_file:\n",
    "            ids = json.load(json_file)\n",
    "    else:\n",
    "        ids = {}\n",
    "        # prendo gli id di babelnet per ogni parola della frase\n",
    "        for word in sentence:\n",
    "            ids[word] = requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "        # prendo i synset di babelnet per ogni parola della frase\n",
    "        with open('data/ids_'+ file_name +'.json', 'w') as fp:\n",
    "            json.dump(ids, fp)\n",
    "\n",
    "    return ids\n",
    "\n",
    "'''\n",
    "Dato un babelnet id guardo nel file locale se ho già le informazioni, altrimenti \n",
    "faccio una richiesta a babelnet e aggiungo la riga al file locale\n",
    "'''\n",
    "def get_babelnet_synset_by_id(syn_id):\n",
    "    df = pd.read_csv('data/local_babelnet_syns.csv')\n",
    "    glosses = ['']\n",
    "    examples = ['']\n",
    "\n",
    "    if syn_id in df['id'].values:\n",
    "        row = df[df['id'] == syn_id]\n",
    "        name = row['name'].values[0]\n",
    "        if not 'nan' in str(row['glosses'].values[0]):\n",
    "            glosses = row['glosses'].values[0].split(';')\n",
    "        if not 'nan' in str(row['examples'].values[0]):\n",
    "            examples = row['examples'].values[0].split(';')             \n",
    "    else:\n",
    "        response = requests.get(f'https://babelnet.io/v8/getSynset?id={syn_id}&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "\n",
    "        print('=============')\n",
    "        print(response)\n",
    "        print('=============')\n",
    "\n",
    "        name = response['senses'][0]['properties']['fullLemma']\n",
    "        glosses = \"\"\n",
    "        for gloss in response['glosses']:\n",
    "            glosses += str(gloss['gloss']) + ';'\n",
    "        examples = \"\"\n",
    "        for example in response['examples']:\n",
    "            examples += str(example['example']) + ';'\n",
    "\n",
    "        #add row to df and save it\n",
    "        df = df.append({'id': syn_id, 'name': name, 'glosses': glosses, 'examples': examples}, ignore_index=True)\n",
    "        df.to_csv('data/local_babelnet_syns.csv', index=False)\n",
    "\n",
    "        glosses = glosses.split(';')\n",
    "        examples = examples.split(';')\n",
    "\n",
    "    return syn_id, name, glosses, examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasari\n",
    "\n",
    "Estrazione dei vettori Nasari dal file locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors():\n",
    "    nasari_vectors = pd.read_csv('data/dd-nasari.txt', on_bad_lines='skip', header=None, sep=';')\n",
    "    nasari_vectors = nasari_vectors.set_index(0)\n",
    "    return nasari_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Simplified Lesk\n",
    "\n",
    "Mi server per fare il WSD dei synsets ottenuti dei token del titolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dati i tutti i synset di una parola e il suo contesto, andando ad applicare\n",
    "il Lesk, restituisce il synset con il contesto più simile\n",
    "'''\n",
    "def get_signature(bn_syn):\n",
    "    _, _, glosses, examples = get_babelnet_synset_by_id(bn_syn)\n",
    "\n",
    "    signature = \"\"\n",
    "    for gloss in glosses:\n",
    "        signature += gloss + ' '\n",
    "    for example in examples:\n",
    "        signature += example + ' '\n",
    "    return set(pre_processing(signature))\n",
    "\n",
    "# Usa come contesto l'intero testo del file, non solo il titolo\n",
    "def simplified_lesk(bn_syns, context):\n",
    "    best_sense = bn_syns[0]['id']\n",
    "    max_overlap = 0\n",
    "    \n",
    "    for bn_syn in bn_syns:\n",
    "        signature = get_signature(bn_syn['id'])\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = bn_syn['id']\n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "def get_best_senses(ids, context):\n",
    "    senses = {}\n",
    "    for word in ids:\n",
    "        senses[word] = simplified_lesk(ids[word], context)\n",
    "\n",
    "    return senses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del documento da tradurre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(file_name):\n",
    "    text = open('data/docs/' + file_name + '.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    # pre processing\n",
    "    text = [line for line in text if line != '']\n",
    "    text_preprocessed = [pre_processing(line) for line in text[0:]]\n",
    "\n",
    "    # prendo il contesto per fare WSD dei babelnet id delle parole del testo\n",
    "    context_for_wsd = []\n",
    "    for sentence in text_preprocessed:\n",
    "        context_for_wsd = context_for_wsd + sentence\n",
    "    context_for_wsd = set(context_for_wsd)\n",
    "\n",
    "    # prendo il titolo\n",
    "    title_preprocessed = text_preprocessed[0]\n",
    "\n",
    "    return title_preprocessed, text_preprocessed, text, context_for_wsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associazione Vectors - Synsets\n",
    "\n",
    "Associazione dei vettori Nasari ai Synset disambiguati delle parole del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors_by_senses(senses):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    vectors = {}\n",
    "\n",
    "    for word in senses:\n",
    "        if senses[word] in list(nasari_vectors.index.values):\n",
    "            #print(f'{senses[word]} in nasari_vectors') \n",
    "            vectors[word] = nasari_vectors.loc[senses[word]]\n",
    "        #else:\n",
    "            #print(f'{senses[word]} not in nasari_vectors')\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_nasari_vector_by_sense(sense):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    vector = None\n",
    "    if sense in list(nasari_vectors.index.values):\n",
    "        vector = nasari_vectors.loc[sense]\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del contesto per fare text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_context(vectors):\n",
    "    get_weighted_context = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        #print(vectors[vector])\n",
    "        sum_weights = 0\n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:\n",
    "                sum_weights += float(word.split('_')[1])\n",
    "            \n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:    \n",
    "                weight = float(word.split('_')[1]) / sum_weights\n",
    "                # preprocess word\n",
    "                word_to_added = word.split('_')[0]\n",
    "                word_to_added = re.sub(r'[^\\w\\s]',' ',word_to_added) #remove punctuation\n",
    "                word_to_added = word_to_added.lower()\n",
    "                word_to_added = lemmatizer.lemmatize(word_to_added)\n",
    "\n",
    "                get_weighted_context.append((word_to_added, weight))\n",
    "\n",
    "    return get_weighted_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesatura dei paragrafi\n",
    "\n",
    "Eseguita usando la metrica Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# weighted overlap\n",
    "def weighted_overlap2(vect1, vect2):\n",
    "    tot = 0.0\n",
    "    overlap = 0\n",
    "    for i, elem in enumerate(vect1):\n",
    "        try:\n",
    "            index = vect2.index(elem) + 1\n",
    "            overlap += 1\n",
    "        except:\n",
    "            index = -1\n",
    "        if index != -1:\n",
    "            tot += (i + 1 + index) ** (-1)\n",
    "    denominatore = 1.0\n",
    "    for i in range(1, overlap + 1):\n",
    "        denominatore += (2 * i) ** (-1)\n",
    "    return tot / denominatore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def disambiguate_word_context(weighted_context):\n",
    "    synsets_topic = []\n",
    "    total_weight = 0\n",
    "    for (word_topic, weight) in weighted_context:\n",
    "        synsets_topic.append((simplified_lesk(get_babelnet_synset_by_word(word_topic), set([tupla[0] for tupla in weighted_context])), weight))\n",
    "        total_weight += weight\n",
    "    return synsets_topic, total_weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_score_sentence(sentence, synsets_context, total_weight):\n",
    "    synsets_sentence = []\n",
    "    score_sentence = 0\n",
    "    for word in sentence:\n",
    "        synsets_sentence.append(simplified_lesk(get_babelnet_synset_by_word(word), set([tupla[0] for tupla in synsets_context])))\n",
    "    for syn_word in synsets_sentence:\n",
    "        score_word = 0\n",
    "        for (syn_topic,weight) in synsets_context:\n",
    "            score_word += (weighted_overlap2(syn_word, syn_topic)*weight)\n",
    "        score_word /= total_weight #media ponderata\n",
    "        score_sentence += score_word\n",
    "    score_sentence /= len(sentence)\n",
    "    return score_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_overlap(sentence, weighted_context):\n",
    "    numeratore = 0\n",
    "    for word in sentence:\n",
    "        #check if word is in first column of key_words\n",
    "        if word in [x[0] for x in weighted_context]:\n",
    "            #print(f'{word} in key_words')\n",
    "            #get index of word in key_words\n",
    "            index = [x[1] for x in weighted_context if x[0] == word][0]\n",
    "            #print(f'index: {index}')\n",
    "\n",
    "            numeratore += 1/(index)\n",
    "            #print('\\n')\n",
    "       # else:\n",
    "            #print(f'{word} not in key_words')\n",
    "            #print('\\n')\n",
    "\n",
    "    i = 1\n",
    "    denominatore = 0\n",
    "    for word in weighted_context:\n",
    "        denominatore += 1/(2*i)\n",
    "        i += 1\n",
    "\n",
    "    return numeratore/denominatore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione automatica del riassunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summarization(text, text_preprocessed, synsets_context, total_weight, perc=0.8):\n",
    "    text_preprocessed = text_preprocessed[1:]\n",
    "\n",
    "    title = text[0]\n",
    "    text = text[1:]\n",
    "\n",
    "    weight_sentences = []\n",
    "    i = 0\n",
    "    for line in text_preprocessed:\n",
    "        # attribuisce un peso ad ogni frase\n",
    "        weight_sentences.append((i, text[i], compute_score_sentence(line, synsets_context, total_weight)))\n",
    "        i += 1\n",
    "\n",
    "    # ordina le frasi in base al peso\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[2], reverse=True)\n",
    "    # prendi il primo 80% delle weight_sentences\n",
    "    weight_sentences = weight_sentences[:round(len(weight_sentences) * perc)]\n",
    "    # ordina le frasi in base all'id\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[0])\n",
    "\n",
    "    # prendi solo le frasi\n",
    "    summary = [x[1] for x in weight_sentences]\n",
    "    #add in the first postizion the title\n",
    "    summary.insert(0, title)\n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    summary = '\\n\\n'.join(summary)\n",
    "    print(summary)\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Andy-Warhol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_preprocessed, text_preprocessed, text, context_for_wsd = open_text(file_name)\n",
    "bn_ids = get_sentence_babelnet_ids(file_name, title_preprocessed)\n",
    "title_sense = get_best_senses(bn_ids, context_for_wsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dd-nasari.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9196\\802201046.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtitle_nasari_vectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_nasari_vectors_by_senses\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtitle_sense\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9196\\974162011.py\u001B[0m in \u001B[0;36mget_nasari_vectors_by_senses\u001B[1;34m(senses)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_nasari_vectors_by_senses\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msenses\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mnasari_vectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_nasari_vectors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[0mvectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msenses\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9196\\961054955.py\u001B[0m in \u001B[0;36mget_nasari_vectors\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_nasari_vectors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mnasari_vectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'data/dd-nasari.txt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mon_bad_lines\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'skip'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m';'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[0mnasari_vectors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnasari_vectors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mnasari_vectors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    676\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    677\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 678\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    679\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    680\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    574\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 575\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    576\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    577\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    930\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    931\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[1;33m|\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 932\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    933\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    934\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1214\u001B[0m             \u001B[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1215\u001B[0m             \u001B[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1216\u001B[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001B[0m\u001B[0;32m   1217\u001B[0m                 \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    784\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    785\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 786\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    787\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    788\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/dd-nasari.txt'"
     ]
    }
   ],
   "source": [
    "title_nasari_vectors = get_nasari_vectors_by_senses(title_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_context = get_weighted_context(title_nasari_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synsets_context, total_weight = disambiguate_word_context(weighted_context=weighted_context)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = make_summarization(text, text_preprocessed, synsets_context, total_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione\n",
    "\n",
    "la valutazione può essere eseguita sulla base di due metriche complementari\n",
    "- BLEU (bilingual evaluation understudy) per quanto riguarda la precision\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) per quanto riguarda la recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "funzione di scoring che è stata elaborata per valutare i sistemi per la traduzione automatica\n",
    "- costruire un sommario di riferimento, come un elenco di termini rilevanti che dovrebbero essere presenti.\n",
    "- confrontare l'insieme di termini nel riepilogo automatico (che chiamiamo riepilogo del candidato) con quelli nel riepilogo del candidato.\n",
    "- il punteggio BLEU è calcolato come P = m/wt che è la frazione di termini del candidato che si trovano nel riferimento, dove m è il numero di termini del candidato che sono nel riferimento, e wt è la dimensione di il candidato\n",
    "\n",
    "La precision in IR è solitamente definita come \n",
    "precision = |{relevant documents} ∩ {retrieved documents}| / |{retrieved documents}|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Precision: {count/len(automatic_sum)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECALL\n",
    "\n",
    "Questa metrica stima in che misura le parole (e/o n-grammi) nei riassunti di riferimento umano sono apparse nei riassunti creati dal sistema\n",
    "- ROUGE-N: Sovrapposizione di N-grammi tra candidato e riferimento\n",
    "riepilogo.\n",
    "- ROUGE-1 si riferisce alla sovrapposizione di unigramma (ogni parola) tra il sommari di sistema e di riferimento.\n",
    "\n",
    "La Recall in IR è abitualmente definito come recall =|{relevant documents} ∩ {retrieved documents}| / |{relevant documents}|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Recall: {count/len(manual_sum)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
