{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 5 - Automatic summarization\n",
    "\n",
    "Si vuole ridurre un documento del 10%, 20% o 30% secondo la seguente strategia\n",
    "\n",
    "1. Individuare l'argomento del testo che si sta riassumendo; l'argomento può essere indicato come un (insieme di) vettori NASARI:\n",
    "    - vt1 = {term1_score, term2_score, …, term10_score }\n",
    "    - vt2 = {term1_score, term2_score, …, term10_score } \n",
    "    - ...\n",
    "\n",
    "2. creare il contesto, raccogliendo qui i vettori dei termini (questo passaggio può essere ripetuto, scaricando ad ogni round il contributo dei termini associati)\n",
    "\n",
    "3. conservare i paragrafi le cui frasi contengono i termini più salienti, in base alla sovrapposizione ponderata, WO(v1,v2)\n",
    "    - riclassificare il peso dei paragrafi applicando almeno uno degli approcci menzionati (titolo, spunto, frase, coesione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "#BABELNET_TOKEN = '1e258739-f5e4-4961-8267-a2da4fe94572' #MO\n",
    "BABELNET_TOKEN = '01a5d861-2f36-45cb-8974-a2a6526530d2' #LT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Metodo utilizzato per eseguire il preprocessing delle frasi, in cui vengono effettuate le seguenti operazioni:\n",
    "- Rimozione della punteggiatura\n",
    "- Trasformazione delle lettere in lowercase\n",
    "- Tokenizzazione della frase tenendo conte delle multiword expression\n",
    "- Lemmatizzazione di tutte le parole\n",
    "- Rimozione delle stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #remove stop words\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_processing(document):\n",
    "    document = re.sub(r'[^\\w\\s]',' ',document) #remove punctuation\n",
    "    document = document.lower()\n",
    "    document = tokenizer.tokenize(document.split())\n",
    "    document = [lemmatizer.lemmatize(token) for token in document]  \n",
    "    document = [w for w in document if not w in stop_words]\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babelnet Id di una frase\n",
    "\n",
    "Viene utilizzato principalmente per ottenere i Babelnet Id delle parole del titolo, che una volta sottosposti a WSD (in quanto, molto probabilmente, per ogni parola avremo più synset) ci serviranno per ottenere i vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data una frase restituisce tutti i suoi babelnet id\n",
    "'''\n",
    "def get_sentence_babelnet_ids(file_name, sentence):\n",
    "    if os.path.exists('data/ids-'+ file_name +'.json'):\n",
    "        with open('data/ids-'+ file_name +'.json') as json_file:\n",
    "            ids = json.load(json_file)\n",
    "    else:\n",
    "        ids = {}\n",
    "        # prendo gli id di babelnet per ogni parola della frase\n",
    "        for word in sentence:\n",
    "            ids[word] = requests.get(f'https://babelnet.io/v8/getSynsetIds?lemma={word}&searchLang=EN&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "        # prendo i synset di babelnet per ogni parola della frase\n",
    "        with open('data/ids_'+ file_name +'.json', 'w') as fp:\n",
    "            json.dump(ids, fp)\n",
    "\n",
    "    return ids\n",
    "\n",
    "'''\n",
    "Dato un babelnet id guardo nel file locale se ho già le informazioni, altrimenti \n",
    "faccio una richiesta a babelnet e aggiungo la riga al file locale\n",
    "'''\n",
    "def get_babelnet_synset_by_id(syn_id):\n",
    "    df = pd.read_csv('data/local_babelnet_syns.csv')\n",
    "    glosses = ['']\n",
    "    examples = ['']\n",
    "\n",
    "    if syn_id in df['id'].values:\n",
    "        row = df[df['id'] == syn_id]\n",
    "        name = row['name'].values[0]\n",
    "        if not 'nan' in str(row['glosses'].values[0]):\n",
    "            glosses = row['glosses'].values[0].split(';')\n",
    "        if not 'nan' in str(row['examples'].values[0]):\n",
    "            examples = row['examples'].values[0].split(';')             \n",
    "    else:\n",
    "        response = requests.get(f'https://babelnet.io/v8/getSynset?id={syn_id}&key={BABELNET_TOKEN}').json()\n",
    "\n",
    "\n",
    "        print('=============')\n",
    "        print(response)\n",
    "        print('=============')\n",
    "\n",
    "        name = response['senses'][0]['properties']['fullLemma']\n",
    "        glosses = \"\"\n",
    "        for gloss in response['glosses']:\n",
    "            glosses += str(gloss['gloss']) + ';'\n",
    "        examples = \"\"\n",
    "        for example in response['examples']:\n",
    "            examples += str(example['example']) + ';'\n",
    "\n",
    "        #add row to df and save it\n",
    "        df = df.concat({'id': syn_id, 'name': name, 'glosses': glosses, 'examples': examples}, ignore_index=True)\n",
    "        df.to_csv('data/local_babelnet_syns.csv', index=False)\n",
    "\n",
    "        glosses = glosses.split(';')\n",
    "        examples = examples.split(';')\n",
    "\n",
    "    return syn_id, name, glosses, examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasari\n",
    "\n",
    "Estrazione dei vettori Nasari dal file locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors():\n",
    "    nasari_vectors = pd.read_csv('data/dd-nasari.txt', on_bad_lines='skip', header=None, sep=';')\n",
    "    nasari_vectors = nasari_vectors.set_index(0)\n",
    "    nasari_vectors[1].fillna('', inplace=True)    \n",
    "\n",
    "    return nasari_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Simplified Lesk per disambiguare il titolo\n",
    "\n",
    "Mi server per fare il WSD dei synsets ottenuti dei token del titolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dati i tutti i synset di una parola e il suo contesto, andando ad applicare\n",
    "il Lesk, restituisce il synset con il contesto più simile\n",
    "'''\n",
    "def get_signature(bn_syn):\n",
    "    _, _, glosses, examples = get_babelnet_synset_by_id(bn_syn)\n",
    "\n",
    "    signature = \"\"\n",
    "    for gloss in glosses:\n",
    "        signature += gloss + ' '\n",
    "    for example in examples:\n",
    "        signature += example + ' '\n",
    "    return set(pre_processing(signature))\n",
    "\n",
    "# Usa come contesto l'intero testo del file, non solo il titolo\n",
    "def simplified_lesk_by_syns(bn_syns, context):\n",
    "    best_sense = None\n",
    "    if bn_syns is not None and len(bn_syns) > 0:\n",
    "        best_sense = bn_syns[0]['id']\n",
    "        max_overlap = 0\n",
    "\n",
    "        for bn_syn in bn_syns:\n",
    "            signature = get_signature(bn_syn['id'])\n",
    "            overlap = len(context.intersection(signature))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = bn_syn['id']\n",
    "    return best_sense\n",
    "\n",
    "def get_best_senses(ids, context):\n",
    "    senses = {}\n",
    "    for word in ids:\n",
    "        senses[word] = simplified_lesk_by_syns(ids[word], context)\n",
    "\n",
    "    return senses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del documento da riassumere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text(file_name):\n",
    "    text = open('data/docs/' + file_name + '.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    # pre processing\n",
    "    text = [line for line in text if line != '']\n",
    "    text_preprocessed = [pre_processing(line) for line in text[0:]]\n",
    "\n",
    "    # prendo il contesto per fare WSD dei babelnet id delle parole del testo\n",
    "    context_for_wsd = []\n",
    "    for sentence in text_preprocessed:\n",
    "        context_for_wsd = context_for_wsd + sentence\n",
    "    context_for_wsd = set(context_for_wsd)\n",
    "\n",
    "    # prendo il titolo\n",
    "    title_preprocessed = text_preprocessed[0]\n",
    "\n",
    "    return title_preprocessed, text_preprocessed, text, context_for_wsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associazione Nasari Vectors - Babelnet Synsets\n",
    "\n",
    "Associazione dei vettori Nasari ai Synset disambiguati delle parole del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Per ogni senso presente nel dizionario senses, restituisce il vettore nasari corrispondente se esiste\n",
    "'''\n",
    "def get_nasari_vectors_by_senses(senses, nasari_vectors):\n",
    "    vectors = {}\n",
    "\n",
    "    for sense in senses:\n",
    "        v = get_nasari_vectors_by_sense(senses[sense], nasari_vectors)\n",
    "        if v is not None:\n",
    "            vectors[sense] = v\n",
    "\n",
    "    return vectors\n",
    "\n",
    "'''\n",
    "Dato un senso (cioè un babelnet id), restituisce il vettore nasari corrispondente se esiste\n",
    "'''\n",
    "def get_nasari_vectors_by_sense(sense, nasari_vectors):\n",
    "    if sense in list(nasari_vectors.index.values):\n",
    "        return nasari_vectors.loc[sense]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_topic(vectors):\n",
    "    get_weighted_context = []\n",
    "\n",
    "    for vector in vectors:\n",
    "        #print(vectors[vector])\n",
    "        sum_weights = 0\n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:\n",
    "                sum_weights += float(word.split('_')[1])\n",
    "            \n",
    "        for word in vectors[vector]:\n",
    "            array = word.split('_')\n",
    "            if len(array) > 1:    \n",
    "                weight = float(word.split('_')[1]) / sum_weights\n",
    "                # preprocess word\n",
    "                word_to_added = word.split('_')[0]\n",
    "                word_to_added = re.sub(r'[^\\w\\s]',' ',word_to_added) #remove punctuation\n",
    "                word_to_added = word_to_added.lower()\n",
    "                word_to_added = lemmatizer.lemmatize(word_to_added)\n",
    "\n",
    "                get_weighted_context.append((word_to_added, weight))\n",
    "\n",
    "    return get_weighted_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribuzione score ai paragrafi\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ottenimento dei vettori nasari delle parole nel paragrafo\n",
    "\n",
    "Data una parola x della frase, andiamo a cercare in nasari_vectors tutti i vettori nasari che hanno x come testa. Fatto ciò, probabilmente, avremo più vettori per ogni parola (quindi più sensi per la parola), occorre quindi fare WSD su di essi. Per fare ciò ricorriamo all'utilizzo di un'altra implementazione del simplified lesk che andrà ad usare come contesto le parole presenti nel topic, mentre come signature le parole presenti nel vettore nasari del senso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Restituisce i vettori nasari che contengono nella testa la parola word\n",
    "'''\n",
    "def get_nasari_vectors_by_token(token, nasari_vectors):\n",
    "    return nasari_vectors[nasari_vectors[1].str.contains(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Per ogni vettore nasari del token, restituisce un dizionario con chiave il babelnet id e valore le parole\n",
    "contenute nel vettore nasari\n",
    "'''\n",
    "def get_context_foreach_token_sense(nasari_vectors_by_token):\n",
    "    context = {}\n",
    "    for index, row in nasari_vectors_by_token.iterrows():\n",
    "        bn_id = row.name\n",
    "        row_values = row.iloc[1:].apply(lambda x: x.split('_')[0] if len(x.split('_')) > 0 else '').values\n",
    "        concatenated_row = ' '.join(row_values)\n",
    "        context[bn_id] = pre_processing(concatenated_row)\n",
    "    \n",
    "    if context == {}:\n",
    "        return None\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:488\u001b[0m, in \u001b[0;36mpandas._libs.index.ObjectEngine._searchsorted_left\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:456\u001b[0m, in \u001b[0;36mpandas._libs.index._bin_search\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:490\u001b[0m, in \u001b[0;36mpandas._libs.index.ObjectEngine._searchsorted_left\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m             weighted_topic_vectors[token] \u001b[39m=\u001b[39m nasari_vectors\u001b[39m.\u001b[39mloc[best_sense]\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m weighted_topic_vectors\n\u001b[1;32m---> 36\u001b[0m weighted_topic_vectors \u001b[39m=\u001b[39m get_nasari_vector_for_topic(title_nasari_vectors, weighted_topic, nasari_vectors)\n",
      "Cell \u001b[1;32mIn[210], line 32\u001b[0m, in \u001b[0;36mget_nasari_vector_for_topic\u001b[1;34m(title_nasari_vectors, weighted_topic, nasari_vectors)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m         best_sense \u001b[39m=\u001b[39m simplified_lesk_for_paragraphs_word(token, context, nasari_vectors)\n\u001b[1;32m---> 32\u001b[0m         weighted_topic_vectors[token] \u001b[39m=\u001b[39m nasari_vectors\u001b[39m.\u001b[39;49mloc[best_sense]\n\u001b[0;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m weighted_topic_vectors\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1312\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexing.py:1260\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[0;32m   1259\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4056\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4054\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[0;32m   4055\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4056\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   4058\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m   4059\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "def get_contexts_of_sentence_word_from_nasari(word, nasari_vectors):\n",
    "    nasari_vectors_by_token = get_nasari_vectors_by_token(word, nasari_vectors)\n",
    "    return get_context_foreach_token_sense(nasari_vectors_by_token)\n",
    "\n",
    "def simplified_lesk_for_paragraphs_word(word, context, nasari_vectors):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    word_signature = get_contexts_of_sentence_word_from_nasari(word, nasari_vectors)\n",
    "    print(word_signature)\n",
    "    if word_signature is not None:\n",
    "        print('b')\n",
    "        if len(word_signature) > 0:\n",
    "            print('c')\n",
    "            for bn_syn in word_signature.keys():\n",
    "                signature = word_signature[bn_syn]\n",
    "                overlap = len(context.intersection(signature))\n",
    "                if overlap > max_overlap:\n",
    "                    max_overlap = overlap\n",
    "                    best_sense = bn_syn\n",
    "    return best_sense\n",
    "\n",
    "def get_nasari_vector_for_topic(title_nasari_vectors, weighted_topic, nasari_vectors):\n",
    "    context = [x[0] for x in weighted_topic]\n",
    "\n",
    "    weighted_topic_vectors = {}\n",
    "    for token, _ in weighted_topic:\n",
    "        if token in title_nasari_vectors:\n",
    "            weighted_topic_vectors[token] = title_nasari_vectors[token]\n",
    "        else:\n",
    "            best_sense = simplified_lesk_for_paragraphs_word(token, context, nasari_vectors)\n",
    "            weighted_topic_vectors[token] = nasari_vectors.loc[best_sense]\n",
    "\n",
    "    return weighted_topic_vectors\n",
    "\n",
    "weighted_topic_vectors = get_nasari_vector_for_topic(title_nasari_vectors, weighted_topic, nasari_vectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pesatura dei paragrafi\n",
    "\n",
    "Eseguita usando la metrica Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weighted overlap\n",
    "def weighted_overlap2(vect1, vect2):\n",
    "    tot = 0.0\n",
    "    overlap = 0\n",
    "    for i, elem in enumerate(vect1):\n",
    "        try:\n",
    "            index = vect2.index(elem) + 1\n",
    "            overlap += 1\n",
    "        except:\n",
    "            index = -1\n",
    "        if index != -1:\n",
    "            tot += (i + 1 + index) ** (-1)\n",
    "    denominatore = 1.0\n",
    "    for i in range(1, overlap + 1):\n",
    "        denominatore += (2 * i) ** (-1)\n",
    "    return tot / denominatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_score_paragraph(paragraph, synsets_context, total_weight, nasari_vectors):\n",
    "    synsets_sentence = []\n",
    "    score_sentence = 0\n",
    "    for word in paragraph:\n",
    "        synsets_sentence.append(simplified_lesk_for_paragraphs_word(word, set([tupla[0] for tupla in synsets_context])), nasari_vectors)\n",
    "    for syn_word in synsets_sentence:\n",
    "        score_word = 0\n",
    "        for (syn_topic,weight) in synsets_context:\n",
    "            score_word += (weighted_overlap2(syn_word, syn_topic)*weight)\n",
    "        score_word /= total_weight #media ponderata\n",
    "        score_sentence += score_word\n",
    "    score_sentence /= len(paragraph)\n",
    "    return score_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def disambiguate_word_context(weighted_context):\n",
    "    synsets_topic = []\n",
    "    total_weight = 0\n",
    "    for (word_topic, weight) in weighted_context:\n",
    "        synset_word_context = simplified_lesk(get_babelnet_synset_by_word(word_topic), set([tupla[0] for tupla in weighted_context if tupla[0] != word_topic]))#disambiguo il termine del topic utilizzando lesk, in cui come contesto di disambiguazione utilizzo il contesto eccetto la parola corrente\n",
    "        print('il synset della parola del contesto:',word_topic, ' è -> ', synset_word_context)\n",
    "        if synset_word_context is not None:\n",
    "            synsets_topic.append((synset_word_context, weight))\n",
    "            total_weight += weight\n",
    "    return synsets_topic, total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def weighted_overlap(sentence, weighted_context):\n",
    "    numeratore = 0\n",
    "    for word in sentence:\n",
    "        #check if word is in first column of key_words\n",
    "        if word in [x[0] for x in weighted_context]:\n",
    "            #print(f'{word} in key_words')\n",
    "            #get index of word in key_words\n",
    "            index = [x[1] for x in weighted_context if x[0] == word][0]\n",
    "            #print(f'index: {index}')\n",
    "\n",
    "            numeratore += 1/(index)\n",
    "            #print('\\n')\n",
    "       # else:\n",
    "            #print(f'{word} not in key_words')\n",
    "            #print('\\n')\n",
    "\n",
    "    i = 1\n",
    "    denominatore = 0\n",
    "    for word in weighted_context:\n",
    "        denominatore += 1/(2*i)\n",
    "        i += 1\n",
    "\n",
    "    return numeratore/denominatore'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione automatica del riassunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summarization(text, text_preprocessed, synsets_context, total_weight, perc=0.8):\n",
    "    text_preprocessed = text_preprocessed[1:]\n",
    "\n",
    "    title = text[0]\n",
    "    text = text[1:] #rimuovo il titolo\n",
    "\n",
    "    weight_sentences = []\n",
    "    i = 0\n",
    "\n",
    "    for paragraph in text_preprocessed:\n",
    "        # attribuisce un peso ad ogni frase\n",
    "        weight_sentences.append((i, text[i], compute_score_paragraph(paragraph, synsets_context, total_weight)))\n",
    "        i += 1\n",
    "\n",
    "    # ordina le frasi in base al peso\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[2], reverse=True)\n",
    "    # prendi il primo 80% delle weight_sentences\n",
    "    weight_sentences = weight_sentences[:round(len(weight_sentences) * perc)]\n",
    "    # ordina le frasi in base all'id\n",
    "    weight_sentences = sorted(weight_sentences, key=lambda tup: tup[0])\n",
    "\n",
    "    # prendi solo le frasi\n",
    "    summary = [x[1] for x in weight_sentences]\n",
    "    #add in the first postizion the title\n",
    "    summary.insert(0, title)\n",
    "    return summary\n",
    "\n",
    "def print_summary(summary):\n",
    "    summary = '\\n\\n'.join(summary)\n",
    "    print(summary)\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Andy-Warhol'\n",
    "nasari_vectors = get_nasari_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_preprocessed, text_preprocessed, text, context_for_wsd_title = open_text(file_name)\n",
    "\n",
    "# prendo il titolo e ottengo i sensi corretti dei token del titolo\n",
    "bn_ids = get_sentence_babelnet_ids(file_name, title_preprocessed)\n",
    "title_senses = get_best_senses(bn_ids, context_for_wsd_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottenimento del topic\n",
    "title_nasari_vectors = get_nasari_vectors_by_senses(title_senses, nasari_vectors)\n",
    "weighted_topic = get_weighted_topic(title_nasari_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andy warhol\n",
      "[('warhol', 0.513940240030623), ('andy warhol', 0.15170791020242733), ('basquiat', 0.07589561144761432), ('pop art', 0.05767039697372272), ('sedgwick', 0.04855553804237688), ('painting', 0.04365134763909842), ('film', 0.039632073135034106), ('artist', 0.036058634122176934), ('art', 0.03288824840692621), ('music', 0.24569128741904467), ('popular music', 0.1448455660313237), ('jazz', 0.11714115099670835), ('song', 0.10419559370418197), ('rock', 0.08939061453164898), ('disco', 0.08708697045315517), ('genre', 0.07561972637854525), ('folk music', 0.07089349347988619), ('hop', 0.06513559700550542), ('painting', 0.42251743635434036), ('paint', 0.14766699098034053), ('art', 0.0757427034816619), ('artist', 0.06631962946263885), ('painter', 0.0648116787439353), ('image', 0.061398834628071185), ('watercolor', 0.059217807193723125), ('jpg', 0.052305827075373804), ('abstract expressionism', 0.050019092079914916), ('mind', 0.25866795394004316), ('thought', 0.19115657284604867), ('human', 0.09372313286634439), ('thinking', 0.09100294569491621), ('brain', 0.08198615946216403), ('consciousness', 0.07580936139025525), ('mental', 0.07119702329776889), ('theory', 0.06848035968485293), ('psychology', 0.06797649081760651), ('kurtzman', 0.38422236875884047), ('mad', 0.24362607658272137), ('jaffee', 0.08702765011753692), ('magazine', 0.07290308616185363), ('playboy', 0.05771137767352426), ('fanzines', 0.04620937174382492), ('humbug', 0.04561036136936793), ('gaines', 0.033307997040184036), ('comic', 0.029381710552146625), ('vienna', 0.19280677009873062), ('potuznik', 0.18547249647390693), ('fm', 0.1437235543018336), ('recording', 0.08533145275035262), ('label', 0.08533145275035262), ('luxxe', 0.08166431593794077), ('pulsinger', 0.08138222849083217), ('robert hood', 0.07221438645980255), ('museumsquartier', 0.07207334273624826)]\n",
      "pop\n",
      "[('warhol', 0.513940240030623), ('andy warhol', 0.15170791020242733), ('basquiat', 0.07589561144761432), ('pop art', 0.05767039697372272), ('sedgwick', 0.04855553804237688), ('painting', 0.04365134763909842), ('film', 0.039632073135034106), ('artist', 0.036058634122176934), ('art', 0.03288824840692621), ('music', 0.24569128741904467), ('popular music', 0.1448455660313237), ('jazz', 0.11714115099670835), ('song', 0.10419559370418197), ('rock', 0.08939061453164898), ('disco', 0.08708697045315517), ('genre', 0.07561972637854525), ('folk music', 0.07089349347988619), ('hop', 0.06513559700550542), ('painting', 0.42251743635434036), ('paint', 0.14766699098034053), ('art', 0.0757427034816619), ('artist', 0.06631962946263885), ('painter', 0.0648116787439353), ('image', 0.061398834628071185), ('watercolor', 0.059217807193723125), ('jpg', 0.052305827075373804), ('abstract expressionism', 0.050019092079914916), ('mind', 0.25866795394004316), ('thought', 0.19115657284604867), ('human', 0.09372313286634439), ('thinking', 0.09100294569491621), ('brain', 0.08198615946216403), ('consciousness', 0.07580936139025525), ('mental', 0.07119702329776889), ('theory', 0.06848035968485293), ('psychology', 0.06797649081760651), ('kurtzman', 0.38422236875884047), ('mad', 0.24362607658272137), ('jaffee', 0.08702765011753692), ('magazine', 0.07290308616185363), ('playboy', 0.05771137767352426), ('fanzines', 0.04620937174382492), ('humbug', 0.04561036136936793), ('gaines', 0.033307997040184036), ('comic', 0.029381710552146625), ('vienna', 0.19280677009873062), ('potuznik', 0.18547249647390693), ('fm', 0.1437235543018336), ('recording', 0.08533145275035262), ('label', 0.08533145275035262), ('luxxe', 0.08166431593794077), ('pulsinger', 0.08138222849083217), ('robert hood', 0.07221438645980255), ('museumsquartier', 0.07207334273624826)]\n",
      "artist\n",
      "[('warhol', 0.513940240030623), ('andy warhol', 0.15170791020242733), ('basquiat', 0.07589561144761432), ('pop art', 0.05767039697372272), ('sedgwick', 0.04855553804237688), ('painting', 0.04365134763909842), ('film', 0.039632073135034106), ('artist', 0.036058634122176934), ('art', 0.03288824840692621), ('music', 0.24569128741904467), ('popular music', 0.1448455660313237), ('jazz', 0.11714115099670835), ('song', 0.10419559370418197), ('rock', 0.08939061453164898), ('disco', 0.08708697045315517), ('genre', 0.07561972637854525), ('folk music', 0.07089349347988619), ('hop', 0.06513559700550542), ('painting', 0.42251743635434036), ('paint', 0.14766699098034053), ('art', 0.0757427034816619), ('artist', 0.06631962946263885), ('painter', 0.0648116787439353), ('image', 0.061398834628071185), ('watercolor', 0.059217807193723125), ('jpg', 0.052305827075373804), ('abstract expressionism', 0.050019092079914916), ('mind', 0.25866795394004316), ('thought', 0.19115657284604867), ('human', 0.09372313286634439), ('thinking', 0.09100294569491621), ('brain', 0.08198615946216403), ('consciousness', 0.07580936139025525), ('mental', 0.07119702329776889), ('theory', 0.06848035968485293), ('psychology', 0.06797649081760651), ('kurtzman', 0.38422236875884047), ('mad', 0.24362607658272137), ('jaffee', 0.08702765011753692), ('magazine', 0.07290308616185363), ('playboy', 0.05771137767352426), ('fanzines', 0.04620937174382492), ('humbug', 0.04561036136936793), ('gaines', 0.033307997040184036), ('comic', 0.029381710552146625), ('vienna', 0.19280677009873062), ('potuznik', 0.18547249647390693), ('fm', 0.1437235543018336), ('recording', 0.08533145275035262), ('label', 0.08533145275035262), ('luxxe', 0.08166431593794077), ('pulsinger', 0.08138222849083217), ('robert hood', 0.07221438645980255), ('museumsquartier', 0.07207334273624826)]\n",
      "thought\n",
      "[('warhol', 0.513940240030623), ('andy warhol', 0.15170791020242733), ('basquiat', 0.07589561144761432), ('pop art', 0.05767039697372272), ('sedgwick', 0.04855553804237688), ('painting', 0.04365134763909842), ('film', 0.039632073135034106), ('artist', 0.036058634122176934), ('art', 0.03288824840692621), ('music', 0.24569128741904467), ('popular music', 0.1448455660313237), ('jazz', 0.11714115099670835), ('song', 0.10419559370418197), ('rock', 0.08939061453164898), ('disco', 0.08708697045315517), ('genre', 0.07561972637854525), ('folk music', 0.07089349347988619), ('hop', 0.06513559700550542), ('painting', 0.42251743635434036), ('paint', 0.14766699098034053), ('art', 0.0757427034816619), ('artist', 0.06631962946263885), ('painter', 0.0648116787439353), ('image', 0.061398834628071185), ('watercolor', 0.059217807193723125), ('jpg', 0.052305827075373804), ('abstract expressionism', 0.050019092079914916), ('mind', 0.25866795394004316), ('thought', 0.19115657284604867), ('human', 0.09372313286634439), ('thinking', 0.09100294569491621), ('brain', 0.08198615946216403), ('consciousness', 0.07580936139025525), ('mental', 0.07119702329776889), ('theory', 0.06848035968485293), ('psychology', 0.06797649081760651), ('kurtzman', 0.38422236875884047), ('mad', 0.24362607658272137), ('jaffee', 0.08702765011753692), ('magazine', 0.07290308616185363), ('playboy', 0.05771137767352426), ('fanzines', 0.04620937174382492), ('humbug', 0.04561036136936793), ('gaines', 0.033307997040184036), ('comic', 0.029381710552146625), ('vienna', 0.19280677009873062), ('potuznik', 0.18547249647390693), ('fm', 0.1437235543018336), ('recording', 0.08533145275035262), ('label', 0.08533145275035262), ('luxxe', 0.08166431593794077), ('pulsinger', 0.08138222849083217), ('robert hood', 0.07221438645980255), ('museumsquartier', 0.07207334273624826)]\n",
      "trump\n",
      "[('warhol', 0.513940240030623), ('andy warhol', 0.15170791020242733), ('basquiat', 0.07589561144761432), ('pop art', 0.05767039697372272), ('sedgwick', 0.04855553804237688), ('painting', 0.04365134763909842), ('film', 0.039632073135034106), ('artist', 0.036058634122176934), ('art', 0.03288824840692621), ('music', 0.24569128741904467), ('popular music', 0.1448455660313237), ('jazz', 0.11714115099670835), ('song', 0.10419559370418197), ('rock', 0.08939061453164898), ('disco', 0.08708697045315517), ('genre', 0.07561972637854525), ('folk music', 0.07089349347988619), ('hop', 0.06513559700550542), ('painting', 0.42251743635434036), ('paint', 0.14766699098034053), ('art', 0.0757427034816619), ('artist', 0.06631962946263885), ('painter', 0.0648116787439353), ('image', 0.061398834628071185), ('watercolor', 0.059217807193723125), ('jpg', 0.052305827075373804), ('abstract expressionism', 0.050019092079914916), ('mind', 0.25866795394004316), ('thought', 0.19115657284604867), ('human', 0.09372313286634439), ('thinking', 0.09100294569491621), ('brain', 0.08198615946216403), ('consciousness', 0.07580936139025525), ('mental', 0.07119702329776889), ('theory', 0.06848035968485293), ('psychology', 0.06797649081760651), ('kurtzman', 0.38422236875884047), ('mad', 0.24362607658272137), ('jaffee', 0.08702765011753692), ('magazine', 0.07290308616185363), ('playboy', 0.05771137767352426), ('fanzines', 0.04620937174382492), ('humbug', 0.04561036136936793), ('gaines', 0.033307997040184036), ('comic', 0.029381710552146625), ('vienna', 0.19280677009873062), ('potuznik', 0.18547249647390693), ('fm', 0.1437235543018336), ('recording', 0.08533145275035262), ('label', 0.08533145275035262), ('luxxe', 0.08166431593794077), ('pulsinger', 0.08138222849083217), ('robert hood', 0.07221438645980255), ('museumsquartier', 0.07207334273624826)]\n",
      "cheap\n",
      "[('warhol', 0.513940240030623), ('andy warhol', 0.15170791020242733), ('basquiat', 0.07589561144761432), ('pop art', 0.05767039697372272), ('sedgwick', 0.04855553804237688), ('painting', 0.04365134763909842), ('film', 0.039632073135034106), ('artist', 0.036058634122176934), ('art', 0.03288824840692621), ('music', 0.24569128741904467), ('popular music', 0.1448455660313237), ('jazz', 0.11714115099670835), ('song', 0.10419559370418197), ('rock', 0.08939061453164898), ('disco', 0.08708697045315517), ('genre', 0.07561972637854525), ('folk music', 0.07089349347988619), ('hop', 0.06513559700550542), ('painting', 0.42251743635434036), ('paint', 0.14766699098034053), ('art', 0.0757427034816619), ('artist', 0.06631962946263885), ('painter', 0.0648116787439353), ('image', 0.061398834628071185), ('watercolor', 0.059217807193723125), ('jpg', 0.052305827075373804), ('abstract expressionism', 0.050019092079914916), ('mind', 0.25866795394004316), ('thought', 0.19115657284604867), ('human', 0.09372313286634439), ('thinking', 0.09100294569491621), ('brain', 0.08198615946216403), ('consciousness', 0.07580936139025525), ('mental', 0.07119702329776889), ('theory', 0.06848035968485293), ('psychology', 0.06797649081760651), ('kurtzman', 0.38422236875884047), ('mad', 0.24362607658272137), ('jaffee', 0.08702765011753692), ('magazine', 0.07290308616185363), ('playboy', 0.05771137767352426), ('fanzines', 0.04620937174382492), ('humbug', 0.04561036136936793), ('gaines', 0.033307997040184036), ('comic', 0.029381710552146625), ('vienna', 0.19280677009873062), ('potuznik', 0.18547249647390693), ('fm', 0.1437235543018336), ('recording', 0.08533145275035262), ('label', 0.08533145275035262), ('luxxe', 0.08166431593794077), ('pulsinger', 0.08138222849083217), ('robert hood', 0.07221438645980255), ('museumsquartier', 0.07207334273624826)]\n"
     ]
    }
   ],
   "source": [
    "weighted_topic_vectors = get_nasari_vector_for_topic(title_nasari_vectors, weighted_topic, nasari_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warhol', 0.513940240030623),\n",
       " ('andy warhol', 0.15170791020242733),\n",
       " ('basquiat', 0.07589561144761432),\n",
       " ('pop art', 0.05767039697372272),\n",
       " ('sedgwick', 0.04855553804237688),\n",
       " ('painting', 0.04365134763909842),\n",
       " ('film', 0.039632073135034106),\n",
       " ('artist', 0.036058634122176934),\n",
       " ('art', 0.03288824840692621),\n",
       " ('music', 0.24569128741904467),\n",
       " ('popular music', 0.1448455660313237),\n",
       " ('jazz', 0.11714115099670835),\n",
       " ('song', 0.10419559370418197),\n",
       " ('rock', 0.08939061453164898),\n",
       " ('disco', 0.08708697045315517),\n",
       " ('genre', 0.07561972637854525),\n",
       " ('folk music', 0.07089349347988619),\n",
       " ('hop', 0.06513559700550542),\n",
       " ('painting', 0.42251743635434036),\n",
       " ('paint', 0.14766699098034053),\n",
       " ('art', 0.0757427034816619),\n",
       " ('artist', 0.06631962946263885),\n",
       " ('painter', 0.0648116787439353),\n",
       " ('image', 0.061398834628071185),\n",
       " ('watercolor', 0.059217807193723125),\n",
       " ('jpg', 0.052305827075373804),\n",
       " ('abstract expressionism', 0.050019092079914916),\n",
       " ('mind', 0.25866795394004316),\n",
       " ('thought', 0.19115657284604867),\n",
       " ('human', 0.09372313286634439),\n",
       " ('thinking', 0.09100294569491621),\n",
       " ('brain', 0.08198615946216403),\n",
       " ('consciousness', 0.07580936139025525),\n",
       " ('mental', 0.07119702329776889),\n",
       " ('theory', 0.06848035968485293),\n",
       " ('psychology', 0.06797649081760651),\n",
       " ('kurtzman', 0.38422236875884047),\n",
       " ('mad', 0.24362607658272137),\n",
       " ('jaffee', 0.08702765011753692),\n",
       " ('magazine', 0.07290308616185363),\n",
       " ('playboy', 0.05771137767352426),\n",
       " ('fanzines', 0.04620937174382492),\n",
       " ('humbug', 0.04561036136936793),\n",
       " ('gaines', 0.033307997040184036),\n",
       " ('comic', 0.029381710552146625),\n",
       " ('vienna', 0.19280677009873062),\n",
       " ('potuznik', 0.18547249647390693),\n",
       " ('fm', 0.1437235543018336),\n",
       " ('recording', 0.08533145275035262),\n",
       " ('label', 0.08533145275035262),\n",
       " ('luxxe', 0.08166431593794077),\n",
       " ('pulsinger', 0.08138222849083217),\n",
       " ('robert hood', 0.07221438645980255),\n",
       " ('museumsquartier', 0.07207334273624826)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'andy warhol': 1            Andy Warhol\n",
       " 2         warhol_2282.46\n",
       " 3     andy warhol_673.75\n",
       " 4        basquiat_337.06\n",
       " 5         pop art_256.12\n",
       " 6        sedgwick_215.64\n",
       " 7        painting_193.86\n",
       " 8            film_176.01\n",
       " 9          artist_160.14\n",
       " 10            art_146.06\n",
       " Name: bn:00004020n, dtype: object,\n",
       " 'pop': 1           Popular music\n",
       " 2           music_1012.14\n",
       " 3     popular music_596.7\n",
       " 4             jazz_482.57\n",
       " 5             song_429.24\n",
       " 6             rock_368.25\n",
       " 7            disco_358.76\n",
       " 8            genre_311.52\n",
       " 9       folk music_292.05\n",
       " 10             hop_268.33\n",
       " Name: bn:00063586n, dtype: object,\n",
       " 'artist': 1                          Painting\n",
       " 2                  painting_1958.55\n",
       " 3                       paint_684.5\n",
       " 4                         art_351.1\n",
       " 5                     artist_307.42\n",
       " 6                    painter_300.43\n",
       " 7                      image_284.61\n",
       " 8                  watercolor_274.5\n",
       " 9                        jpg_242.46\n",
       " 10    abstract expressionism_231.86\n",
       " Name: bn:00060201n, dtype: object,\n",
       " 'thought': 1                  Thought\n",
       " 2              mind_734.11\n",
       " 3           thought_542.51\n",
       " 4             human_265.99\n",
       " 5          thinking_258.27\n",
       " 6             brain_232.68\n",
       " 7     consciousness_215.15\n",
       " 8            mental_202.06\n",
       " 9            theory_194.35\n",
       " 10       psychology_192.92\n",
       " Name: bn:00017339n, dtype: object,\n",
       " 'trump': 1     Trump (magazine)\n",
       " 2       kurtzman_763.3\n",
       " 3           mad_483.99\n",
       " 4        jaffee_172.89\n",
       " 5      magazine_144.83\n",
       " 6       playboy_114.65\n",
       " 7        fanzines_91.8\n",
       " 8         humbug_90.61\n",
       " 9         gaines_66.17\n",
       " 10         comic_58.37\n",
       " Name: bn:03266978n, dtype: object,\n",
       " 'cheap': 1            Cheap Records\n",
       " 2             vienna_13.67\n",
       " 3           potuznik_13.15\n",
       " 4                 fm_10.19\n",
       " 5           recording_6.05\n",
       " 6               label_6.05\n",
       " 7               luxxe_5.79\n",
       " 8           pulsinger_5.77\n",
       " 9         robert hood_5.12\n",
       " 10    museumsquartier_5.11\n",
       " Name: bn:02213915n, dtype: object}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_nasari_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_score_paragraph(text_preprocessed[1], title_nasari_vectors, 1, nasari_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Your key is not valid or the daily requests limit has been reached. Please visit http://babelnet.org.'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2980\\2790413097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msynsets_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisambiguate_word_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweighted_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2980\\3286270220.py\u001b[0m in \u001b[0;36mdisambiguate_word_context\u001b[1;34m(weighted_context)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtotal_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweighted_context\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msynset_word_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimplified_lesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_babelnet_synset_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_topic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtupla\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtupla\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweighted_context\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtupla\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mword_topic\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#disambiguo il termine del topic utilizzando lesk, in cui come contesto di disambiguazione utilizzo il contesto eccetto la parola corrente\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'il synset della parola del contesto:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' è -> '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset_word_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msynset_word_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2980\\2978368563.py\u001b[0m in \u001b[0;36msimplified_lesk\u001b[1;34m(bn_syns, context)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbn_syns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbn_syns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbn_syns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mbest_sense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbn_syns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mmax_overlap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "synsets_context, total_weight = disambiguate_word_context(weighted_context=weighted_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = make_summarization(text, text_preprocessed, synsets_context, total_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione\n",
    "\n",
    "la valutazione può essere eseguita sulla base di due metriche complementari\n",
    "- BLEU (bilingual evaluation understudy) per quanto riguarda la precision\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) per quanto riguarda la recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "funzione di scoring che è stata elaborata per valutare i sistemi per la traduzione automatica\n",
    "- costruire un sommario di riferimento, come un elenco di termini rilevanti che dovrebbero essere presenti.\n",
    "- confrontare l'insieme di termini nel riepilogo automatico (che chiamiamo riepilogo del candidato) con quelli nel riepilogo del candidato.\n",
    "- il punteggio BLEU è calcolato come P = m/wt che è la frazione di termini del candidato che si trovano nel riferimento, dove m è il numero di termini del candidato che sono nel riferimento, e wt è la dimensione di il candidato\n",
    "\n",
    "La precision in IR è solitamente definita come \n",
    "precision = |{relevant documents} ∩ {retrieved documents}| / |{retrieved documents}|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Precision: {count/len(automatic_sum)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECALL\n",
    "\n",
    "Questa metrica stima in che misura le parole (e/o n-grammi) nei riassunti di riferimento umano sono apparse nei riassunti creati dal sistema\n",
    "- ROUGE-N: Sovrapposizione di N-grammi tra candidato e riferimento\n",
    "riepilogo.\n",
    "- ROUGE-1 si riferisce alla sovrapposizione di unigramma (ogni parola) tra il sommari di sistema e di riferimento.\n",
    "\n",
    "La Recall in IR è abitualmente definito come recall =|{relevant documents} ∩ {retrieved documents}| / |{relevant documents}|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_manual_sum = 'Andy-Warhol-manual-sum'\n",
    "\n",
    "_, _, manual_sum, _ = open_text(file_name_manual_sum)\n",
    "automatic_sum = make_summarization(text, text_preprocessed, weighted_context)\n",
    "\n",
    "count = 0\n",
    "for line in automatic_sum:\n",
    "    if (line in manual_sum):\n",
    "        count += 1\n",
    "\n",
    "print(f'Recall: {count/len(manual_sum)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PENSO NON SERVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vector_by_sense(sense):\n",
    "    nasari_vectors = get_nasari_vectors()\n",
    "    vector = None\n",
    "    if sense in list(nasari_vectors.index.values):\n",
    "        vector = nasari_vectors.loc[sense]\n",
    "    return vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
