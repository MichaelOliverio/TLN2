{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: annotazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame OLIVERio:\n",
    "- [x] Concessive\n",
    "- [x] History\n",
    "- [x] Change_resistance\n",
    "- [x] Emptying\n",
    "- [x] Performers_and_roles\n",
    "\n",
    "Frame TOMATIS\n",
    "- [x] Deciding\n",
    "- [x] Intentionally_act\n",
    "- [x] Competition\n",
    "- [x] Fairness_evaluation\n",
    "- [x] Process_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "frame_names = [\n",
    "    'Concessive',\n",
    "    'History',\n",
    "    'Change_resistance',\n",
    "    'Emptying',\n",
    "    'Performers_and_roles',\n",
    "    'Deciding',\n",
    "    'Intentionally_act',\n",
    "    'Competition',\n",
    "    'Fairness_evaluation',\n",
    "    'Process_continue',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione tokenizzatore per le multiword expressions\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "\n",
    "def make_set(sentence):\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence) #remove punctuation\n",
    "    sentence = tokenizer.tokenize(sentence.split()) #tokenize\n",
    "    sentence = [w for w in sentence if not w.isdigit()] #remove numbers\n",
    "    stop_words = set(stopwords.words('english')) #remove stop words\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer() #lemmatization of definition\n",
    "    sentence = [lemmatizer.lemmatize(w) for w in sentence]\n",
    "\n",
    "    res = []\n",
    "    for w in sentence:\n",
    "        res.append(w.replace(' ', '_'))\n",
    "\n",
    "    return set(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: mapping automatico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio bag of words\n",
    "\n",
    "Scelta del senso che permette di massimizzare l'intersezione tra i contesti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_frame_name_fn(frame):\n",
    "    name = frame.name\n",
    "    definition = frame.definition\n",
    "    FEs = frame.FE\n",
    "    LUs = frame.lexUnit\n",
    "\n",
    "    sentence = name + ' ' + definition\n",
    "    for fe in FEs:\n",
    "        sentence += ' ' + FEs[fe].definition\n",
    "    for lu in LUs:\n",
    "        sentence += ' ' + LUs[lu].definition\n",
    "\n",
    "    return make_set(sentence)\n",
    "\n",
    "def ctx_frame_element_fn(frame_element):\n",
    "    name = frame_element.name\n",
    "    definition = frame_element.definition\n",
    "    #semtype = frame_element.semType.name\n",
    "\n",
    "    sentence = name + ' ' + definition # + ' ' + semtype\n",
    "   \n",
    "    return make_set(sentence)\n",
    "\n",
    "def ctx_lexical_unit_fn(lexical_unit):\n",
    "    name = lexical_unit.name\n",
    "    definition = lexical_unit.definition\n",
    "    exemplars = lexical_unit.exemplars\n",
    "    \n",
    "    sentence = name + ' ' + definition\n",
    "    for ex in exemplars:\n",
    "        sentence += ' ' + ex.annotationSet[0].text\n",
    "\n",
    "    return make_set(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crezione del contesto (WordNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(synset):\n",
    "    hyponyms = set()\n",
    "    for hyponym in synset.hyponyms():\n",
    "        hyponyms |= set(get_hyponyms(hyponym))\n",
    "    return hyponyms | set(synset.hyponyms())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE_SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(synset, depth=1):\n",
    "    sentence = synset.definition()\n",
    "    for example in synset.examples():\n",
    "        sentence += ' ' + example\n",
    "    for lemma in synset.lemmas():\n",
    "        sentence += ' ' + lemma.name()\n",
    "\n",
    "    if (depth >= 0):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            sentence += ' ' + create_sentences(hypernym, depth-1)\n",
    "        for hyponym in get_hyponyms(synset):\n",
    "            sentence += ' ' + create_sentences(hyponym, depth-1)  \n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def ctx_synset_WN(synset):\n",
    "    sentence = create_sentences(synset)\n",
    "    return make_set(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG_OF_WORDS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(ctx_fn):\n",
    "    mappings = {}\n",
    "    for key in ctx_fn:\n",
    "        token = key.split('.')[0]\n",
    "        syns = wn.synsets(token)\n",
    "        max_overlap = 0\n",
    "\n",
    "        # dati i synset andiamo a prendere quello più accurato con il frame\n",
    "        for syn in syns:\n",
    "            overlap = len(ctx_synset_WN(syn).intersection(ctx_fn[key])) + 1\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                mappings[key] = syn\n",
    "\n",
    "    return mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creazione contesto frame net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_fn(frame):\n",
    "    ctx_fn = {}\n",
    "\n",
    "    #frame\n",
    "    ctx_fn[frame.name.replace(' ', '_')] = ctx_frame_name_fn(frame)\n",
    "\n",
    "    #FEs\n",
    "    for fe in frame.FE:\n",
    "        ctx_fn[fe.replace(' ', '_')] = ctx_frame_element_fn(frame.FE[fe])\n",
    "        break\n",
    "\n",
    "    #LUs\n",
    "    for lu in frame.lexUnit:\n",
    "        ctx_fn[lu.replace(' ', '_')] = ctx_lexical_unit_fn(frame.lexUnit[lu])\n",
    "\n",
    "    return ctx_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bag of words\n",
    "\n",
    "Non vengono associati i synset a tutti i token perchè alcuni non hanno alcun synset su wordnet (es. if, although, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concessive {'Concessive': Synset('concessive.a.01'), 'despite.prep': Synset('contempt.n.01'), 'though.scon': Synset('though.r.01'), 'while.scon': Synset('while.n.01'), 'much_as.scon': Synset('much_as.r.01')}\n",
      "\n",
      "\n",
      "History {'History': Synset('history.n.05'), 'Topic': Synset('topic.n.02'), 'history.n': Synset('history.n.05')}\n",
      "\n",
      "\n",
      "Change_resistance {'Agent': Synset('agent.n.01'), 'buttress.v': Synset('buttress.v.01'), 'bulwark.v': Synset('bulwark.v.01'), 'brace.v': Synset('brace.n.01')}\n",
      "\n",
      "\n",
      "Emptying {'Emptying': Synset('empty.v.01'), 'Agent': Synset('agent.n.01'), 'clear.v': Synset('unclutter.v.01'), 'drain.v': Synset('drain.n.02'), 'empty.v': Synset('empty.v.01'), 'purge.v': Synset('purify.v.02'), 'strip.v': Synset('strip.n.02'), 'divest.v': Synset('strip.v.13'), 'rid.v': Synset('rid.v.01'), 'disembowel.v': Synset('disembowel.v.01'), 'skin.v': Synset('skin.n.01'), 'core.v': Synset('kernel.n.03'), 'peel.v': Synset('peel.n.02'), 'gut.v': Synset('gut.v.02'), 'bone.v': Synset('bone.n.02'), 'debug.v': Synset('debug.v.01'), 'deforest.v': Synset('deforest.v.01'), 'degrease.v': Synset('degrease.v.01'), 'descale.v': Synset('scale.v.06'), 'scalp.v': Synset('scalp.v.02'), 'defrost.v': Synset('defrost.v.01'), 'delouse.v': Synset('delouse.v.01'), 'eviscerate.v': Synset('resect.v.01'), 'denude.v': Synset('denude.v.01'), 'unload.v': Synset('unload.v.02'), 'emptying.n': Synset('empty.v.01'), 'expurgate.v': Synset('bowdlerize.v.01'), 'void.v': Synset('invalidate.v.04'), 'evacuate.v': Synset('evacuate.v.01'), 'debone.v': Synset('bone.v.02'), 'devein.v': Synset('devein.v.01'), 'disarm.v': Synset('disarm.v.03'), 'disarmament.n': Synset('disarming.n.01'), 'decontaminate.v': Synset('decontaminate.v.01'), 'decontamination.n': Synset('decontamination.n.01'), 'cleanse.v': Synset('cleanse.v.01'), 'pit.v': Synset('pit.v.03'), 'seed.v': Synset('seed.v.08'), 'stone.v': Synset('rock.n.01'), 'stalk.v': Synset('stalk.n.02'), 'flush.v': Synset('flower.n.03'), 'expunge.v': Synset('strike.v.14'), 'weed.v': Synset('weed.v.01')}\n",
      "\n",
      "\n",
      "Performers_and_roles {'Audience': Synset('audience.n.01'), 'star.v': Synset('ace.n.03'), 'act.v': Synset('act.n.02'), 'play.v': Synset('turn.n.03'), 'be.v': Synset('beryllium.n.01'), 'star.n': Synset('ace.n.03'), 'co-star.n': Synset('co-star.n.01'), 'lead.n': Synset('precede.v.04'), 'as.prep': Synset('arsenic.n.02'), 'co-star.v': Synset('co-star.n.01'), 'feature.v': Synset('feature.n.02'), 'character.n': Synset('character.n.04'), 'role.n': Synset('role.n.04'), 'part.n': Synset('part.n.02'), 'appear.v': Synset('look.v.02')}\n",
      "\n",
      "\n",
      "Deciding {'Deciding': Synset('decision_making.n.01'), 'decide.v': Synset('decide.v.02'), 'decision.n': Synset('decision.n.01'), 'rule_out.v': Synset('rule_out.v.02'), 'determine.v': Synset('determine.v.02')}\n",
      "\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_11472\\3085725460.py\", line 4, in <module>\n",
      "    print(frame_name, bag_of_words(ctx_fn))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_11472\\2514460105.py\", line 10, in bag_of_words\n",
      "    overlap = len(ctx_synset_WN(syn).intersection(ctx_fn[key])) + 1\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_11472\\1374051683.py\", line 19, in ctx_synset_WN\n",
      "    return make_set(sentence)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_11472\\2863402790.py\", line 14, in make_set\n",
      "    sentence = [lemmatizer.lemmatize(w) for w in sentence]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_11472\\2863402790.py\", line 14, in <listcomp>\n",
      "    sentence = [lemmatizer.lemmatize(w) for w in sentence]\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\stem\\wordnet.py\", line 45, in lemmatize\n",
      "    lemmas = wn._morphy(word, pos)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line 2039, in _morphy\n",
      "    results = filter_forms([form] + forms)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\corpus\\reader\\wordnet.py\", line -1, in filter_forms\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "                                              ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\OliverioM\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "for frame_name in frame_names:\n",
    "    frame = fn.frame_by_name(frame_name)\n",
    "    ctx_fn = get_context_fn(frame)\n",
    "    print(frame_name, bag_of_words(ctx_fn))\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_fn_2(frame):\n",
    "    ctx_fn = {}\n",
    "\n",
    "    #FEs\n",
    "    for fe in frame.FE: \n",
    "        ctx_fn[fe.replace(' ', '_')] = ctx_frame_element_fn(frame.FE[fe])\n",
    "    #LUs\n",
    "    for lu in frame.lexUnit:\n",
    "        ctx_fn[lu.replace(' ', '_')] = ctx_lexical_unit_fn(frame.lexUnit[lu])\n",
    "\n",
    "    return ctx_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_between_synsets(synset1, synset2, L=3):\n",
    "    paths = []\n",
    "    visited = set()\n",
    "\n",
    "    def dfs(synset, path):\n",
    "        if synset in visited or len(path) > L:\n",
    "            return\n",
    "        if synset == synset2:\n",
    "            paths.append(path + [synset])\n",
    "            return\n",
    "        visited.add(synset)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            dfs(hypernym, path + [synset])\n",
    "        for hyponym in synset.hyponyms():\n",
    "            dfs(hyponym, path + [synset])\n",
    "\n",
    "    dfs(synset1, [])\n",
    "    return [path for path in paths if len(path) <= L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m             max_syn \u001b[39m=\u001b[39m syn\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m max_syn\n\u001b[1;32m---> 34\u001b[0m argmax_prob(\u001b[39m'\u001b[39;49m\u001b[39mDeciding\u001b[39;49m\u001b[39m'\u001b[39;49m, get_context_fn(fn\u001b[39m.\u001b[39;49mframe_by_name(\u001b[39m'\u001b[39;49m\u001b[39mDeciding\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n",
      "Cell \u001b[1;32mIn[12], line 27\u001b[0m, in \u001b[0;36margmax_prob\u001b[1;34m(word_fn, ctx_fn)\u001b[0m\n\u001b[0;32m     25\u001b[0m max_syn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m syn \u001b[39min\u001b[39;00m wn\u001b[39m.\u001b[39msynsets(word_fn):\n\u001b[1;32m---> 27\u001b[0m     prob_syn \u001b[39m=\u001b[39m prob(syn, word_fn, ctx_fn)\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m prob_syn \u001b[39m>\u001b[39m max_prob:\n\u001b[0;32m     29\u001b[0m         max_prob \u001b[39m=\u001b[39m prob_syn\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mprob\u001b[1;34m(syn_fn, word_fn, ctx_fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m ctx_fn:\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m syn \u001b[39min\u001b[39;00m wn\u001b[39m.\u001b[39msynsets(key):\n\u001b[1;32m---> 19\u001b[0m         \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m score(syn, key, ctx_fn)\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m score(syn_fn, word_fn, ctx_fn) \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m\n",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m, in \u001b[0;36mscore\u001b[1;34m(syn_fn, word_fn, ctx_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m ctx_word:\n\u001b[0;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m syn \u001b[39min\u001b[39;00m wn\u001b[39m.\u001b[39msynsets(word):\n\u001b[0;32m      7\u001b[0m         \u001b[39m#get all connection path between syn and syn_fn\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m         paths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(get_paths_between_synsets(syn, syn_fn, \u001b[39m3\u001b[39;49m))\n\u001b[0;32m     10\u001b[0m         \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths:\n\u001b[0;32m     11\u001b[0m             res \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(path)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mget_paths_between_synsets\u001b[1;34m(synset1, synset2, L)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m hyponym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhyponyms():\n\u001b[0;32m     15\u001b[0m         dfs(hyponym, path \u001b[39m+\u001b[39m [synset])\n\u001b[1;32m---> 17\u001b[0m dfs(synset1, [])\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m [path \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(path) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m L]\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mget_paths_between_synsets.<locals>.dfs\u001b[1;34m(synset, path)\u001b[0m\n\u001b[0;32m     11\u001b[0m visited\u001b[39m.\u001b[39madd(synset)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m hypernym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhypernyms():\n\u001b[1;32m---> 13\u001b[0m     dfs(hypernym, path \u001b[39m+\u001b[39;49m [synset])\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m hyponym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhyponyms():\n\u001b[0;32m     15\u001b[0m     dfs(hyponym, path \u001b[39m+\u001b[39m [synset])\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mget_paths_between_synsets.<locals>.dfs\u001b[1;34m(synset, path)\u001b[0m\n\u001b[0;32m     11\u001b[0m visited\u001b[39m.\u001b[39madd(synset)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m hypernym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhypernyms():\n\u001b[1;32m---> 13\u001b[0m     dfs(hypernym, path \u001b[39m+\u001b[39;49m [synset])\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m hyponym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhyponyms():\n\u001b[0;32m     15\u001b[0m     dfs(hyponym, path \u001b[39m+\u001b[39m [synset])\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mget_paths_between_synsets.<locals>.dfs\u001b[1;34m(synset, path)\u001b[0m\n\u001b[0;32m     13\u001b[0m     dfs(hypernym, path \u001b[39m+\u001b[39m [synset])\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m hyponym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhyponyms():\n\u001b[1;32m---> 15\u001b[0m     dfs(hyponym, path \u001b[39m+\u001b[39;49m [synset])\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mget_paths_between_synsets.<locals>.dfs\u001b[1;34m(synset, path)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m visited\u001b[39m.\u001b[39madd(synset)\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfor\u001b[39;00m hypernym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39;49mhypernyms():\n\u001b[0;32m     13\u001b[0m     dfs(hypernym, path \u001b[39m+\u001b[39m [synset])\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m hyponym \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mhyponyms():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\corpus\\reader\\wordnet.py:130\u001b[0m, in \u001b[0;36m_WordNetObject.hypernyms\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhypernyms\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_related(\u001b[39m\"\u001b[39;49m\u001b[39m@\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1097\u001b[0m, in \u001b[0;36mSynset._related\u001b[1;34m(self, relation_symbol, sort)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m   1096\u001b[0m pointer_tuples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pointers[relation_symbol]\n\u001b[1;32m-> 1097\u001b[0m r \u001b[39m=\u001b[39m [get_synset(pos, offset) \u001b[39mfor\u001b[39;49;00m pos, offset \u001b[39min\u001b[39;49;00m pointer_tuples]\n\u001b[0;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m sort:\n\u001b[0;32m   1099\u001b[0m     r\u001b[39m.\u001b[39msort()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1097\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m   1096\u001b[0m pointer_tuples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pointers[relation_symbol]\n\u001b[1;32m-> 1097\u001b[0m r \u001b[39m=\u001b[39m [get_synset(pos, offset) \u001b[39mfor\u001b[39;00m pos, offset \u001b[39min\u001b[39;00m pointer_tuples]\n\u001b[0;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m sort:\n\u001b[0;32m   1099\u001b[0m     r\u001b[39m.\u001b[39msort()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1479\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synset_from_pos_and_offset\u001b[1;34m(self, pos, offset)\u001b[0m\n\u001b[0;32m   1476\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_file_map[pos] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(fileid)\n\u001b[0;32m   1477\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_file_map[pos]\n\u001b[1;32m-> 1479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynset_from_pos_and_offset\u001b[39m(\u001b[39mself\u001b[39m, pos, offset):\n\u001b[0;32m   1480\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[39m    - pos: The synset's part of speech, matching one of the module level\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[39m      attributes ADJ, ADJ_SAT, ADV, NOUN or VERB ('a', 's', 'r', 'n', or 'v').\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[39m    Synset('entity.n.01')\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m     \u001b[39m# Check to see if the synset is in the cache\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def score(syn_fn, word_fn, ctx_fn):\n",
    "    ctx_word = ctx_fn[word_fn]\n",
    "\n",
    "    res = 0\n",
    "    for word in ctx_word:\n",
    "        for syn in wn.synsets(word):\n",
    "            #get all connection path between syn and syn_fn\n",
    "            paths = list(get_paths_between_synsets(syn, syn_fn, 3))\n",
    "\n",
    "            for path in paths:\n",
    "                res += np.exp(-len(path)-1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def prob(syn_fn, word_fn, ctx_fn):\n",
    "    sum = 0\n",
    "    for key in ctx_fn:\n",
    "        for syn in wn.synsets(key):\n",
    "            sum += score(syn, key, ctx_fn)\n",
    "\n",
    "    return score(syn_fn, word_fn, ctx_fn) / sum\n",
    "\n",
    "def argmax_prob(word_fn, ctx_fn):\n",
    "    max_prob = 0\n",
    "    max_syn = None\n",
    "    for syn in wn.synsets(word_fn):\n",
    "        prob_syn = prob(syn, word_fn, ctx_fn)\n",
    "        if prob_syn > max_prob:\n",
    "            max_prob = prob_syn\n",
    "            max_syn = syn\n",
    "            \n",
    "    return max_syn\n",
    "\n",
    "argmax_prob('Deciding', get_context_fn(fn.frame_by_name('Deciding')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: valutazione\n",
    "\n",
    "La funzionalità di valutazione confronta i synset restituiti in output dal sistema con quelli annotati a mano; su questa base deve essere calcolata l'accuratezza del sistema, semplicemente come rapporto degli elementi corretti sul totale degli elementi.\n",
    "\n",
    "Opzionale\n",
    "- Confronto fra l'output dei due approcci descritti (bag-of-words e con grafo).\n",
    "- Sviluppo di metriche che considerino anche la distanza semantica fra eventuali synset errati e corretti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_11472\\2018358456.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  corpus = pd.read_csv('frame_annotati.csv', error_bad_lines=False)\n",
      "Skipping line 21: expected 3 fields, saw 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('frame_annotati.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words2(key, ctx_fn):\n",
    "    best_syn = None\n",
    "\n",
    "    token = key.split('.')[0]\n",
    "    syns = wn.synsets(token)\n",
    "    max_overlap = 0\n",
    "\n",
    "    # dati i synset andiamo a prendere quello più accurato con il frame\n",
    "    for syn in syns:\n",
    "        overlap = len(ctx_synset_WN(syn).intersection(ctx_fn)) + 1\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_syn = syn\n",
    "\n",
    "    return best_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decide.v.01\n",
      "Predicted: decision_making.n.01 actual: decide.v.01\n",
      "None\n",
      "decision.n.01\n",
      "Predicted: decision.n.01 actual: decision.n.01\n",
      "topographic_point.n.01\n",
      "Predicted: topographic_point.n.01 actual: topographic_point.n.01\n",
      "possibility.n.02\n",
      "Predicted: hypothesis.n.02 actual: possibility.n.02\n",
      "time.n.01\n",
      "Predicted: time.n.05 actual: time.n.01\n",
      "circumstance.n.01\n",
      "Predicted: circumstance.n.01 actual: circumstance.n.01\n",
      "explanation.n.03\n",
      "Predicted: explanation.n.02 actual: explanation.n.03\n",
      "purpose.n.01\n",
      "manner.n.01\n",
      "Predicted: manner.n.01 actual: manner.n.01\n",
      "decide.v.01\n",
      "Predicted: decide.v.02 actual: decide.v.01\n",
      "determine.v.03\n",
      "Predicted: determine.v.02 actual: determine.v.03\n",
      "rule_out.v.02\n",
      "Predicted: rule_out.v.02 actual: rule_out.v.02\n",
      " None\n",
      "concessive.a.01\n",
      "Predicted: concessive.a.01 actual: concessive.a.01\n",
      "situation.n.01\n",
      "subject.n.01\n",
      "Predicted: topic.n.02 actual: subject.n.01\n",
      " None\n",
      "emptying.n.01\n",
      "Predicted: empty.v.01 actual: emptying.n.01\n",
      "agent.n.01\n",
      "Predicted: agent.n.01 actual: agent.n.01\n",
      "subject.n.01\n",
      "Predicted: subject.n.01 actual: subject.n.01\n",
      "source.v.02\n",
      "Predicted: reservoir.n.04 actual: source.v.02\n",
      "path.n.02\n",
      "Predicted: path.n.03 actual: path.n.02\n",
      "goal.n.01\n",
      "Predicted: goal.n.01 actual: goal.n.01\n",
      "degree.n.01\n",
      "Predicted: degree.n.02 actual: degree.n.01\n",
      "delineative.s.01\n",
      "Predicted: delineative.s.01 actual: delineative.s.01\n",
      "manner.n.01\n",
      "Predicted: manner.n.01 actual: manner.n.01\n",
      "mean.v.07\n",
      "Predicted: means.n.02 actual: mean.v.07\n",
      "result.v.01\n",
      "Predicted: consequence.n.01 actual: result.v.01\n",
      "instrument.n.02\n",
      "Predicted: instrument.n.01 actual: instrument.n.02\n",
      " None\n",
      "resistance.n.01\n",
      "agent.n.01\n",
      "Predicted: agent.n.01 actual: agent.n.01\n",
      "patient.a.01\n",
      "Predicted: affected_role.n.01 actual: patient.a.01\n",
      "delineative.s.01\n",
      "Predicted: delineative.s.01 actual: delineative.s.01\n",
      "consequence.n.01\n",
      "Predicted: consequence.n.01 actual: consequence.n.01\n",
      "means.n.01\n",
      "Predicted: means.n.02 actual: means.n.01\n",
      "manner.n.01\n",
      "Predicted: manner.n.01 actual: manner.n.01\n",
      "time.n.01\n",
      "Predicted: time.n.05 actual: time.n.01\n",
      "topographic_point.n.01\n",
      "Predicted: place.n.13 actual: topographic_point.n.01\n",
      "cause.n.01\n",
      "Predicted: causal_agent.n.01 actual: cause.n.01\n",
      "attack.n.05\n",
      "Predicted: attack.v.03 actual: attack.n.05\n",
      "bulwark.v.01\n",
      "Predicted: bulwark.v.01 actual: bulwark.v.01\n",
      "brace.n.01\n",
      "Predicted: stimulate.v.04 actual: brace.n.01\n",
      " None\n",
      "None\n",
      "time.n.01\n",
      "Predicted: time.n.05 actual: time.n.01\n",
      "topographic_point.n.01\n",
      "Predicted: place.n.13 actual: topographic_point.n.01\n",
      "co-star.n.01\n",
      "Predicted: co-star.n.01 actual: co-star.n.01\n",
      "co-star.v.01\n",
      "Predicted: co-star.n.01 actual: co-star.v.01\n",
      "star.n.04\n",
      "Predicted: precede.v.04 actual: star.n.04\n",
      " None\n",
      "competition.n.03\n",
      "Predicted: competition.n.03 actual: competition.n.03\n",
      "None\n",
      "None\n",
      "player.n.01\n",
      "Predicted: player.n.01 actual: player.n.01\n",
      "contest.n.01\n",
      "Predicted: contest.n.01 actual: contest.n.01\n",
      "score.n.03\n",
      "Predicted: score.n.03 actual: score.n.03\n",
      "rank.v.01\n",
      "Predicted: rank.v.01 actual: rank.v.01\n",
      "prize.n.01\n",
      "Predicted: trophy.n.02 actual: prize.n.01\n",
      "venue.n.01\n",
      "Predicted: venue.n.01 actual: venue.n.01\n",
      "place.n.02\n",
      "Predicted: place.n.03 actual: place.n.02\n",
      "manner.n.01\n",
      "Predicted: manner.n.01 actual: manner.n.01\n",
      "time.n.03\n",
      "Predicted: time.n.05 actual: time.n.03\n",
      "duration.n.01\n",
      "Predicted: duration.n.01 actual: duration.n.01\n",
      "purpose.n.01\n",
      "Predicted: purpose.n.01 actual: purpose.n.01\n",
      "degree.n.01\n",
      "Predicted: degree.n.02 actual: degree.n.01\n",
      "frequency.n.01\n",
      "Predicted: frequency.n.01 actual: frequency.n.01\n",
      " None\n",
      " None\n",
      " None\n",
      " None\n",
      "degree.n.07\n",
      "Predicted: degree.n.02 actual: degree.n.07\n",
      "manner.n.02\n",
      "Predicted: manner.n.01 actual: manner.n.02\n",
      " None\n",
      " None\n",
      "manner.n.01\n",
      "Predicted: manner.n.01 actual: manner.n.01\n",
      "duration.n.01\n",
      "Predicted: duration.n.01 actual: duration.n.01\n",
      "place.n.02\n",
      "Predicted: place.n.03 actual: place.n.02\n",
      "time.n.03\n",
      "Predicted: time.n.05 actual: time.n.03\n",
      " None\n",
      " None\n",
      " None\n",
      "place.n.02\n",
      "Predicted: place.n.03 actual: place.n.02\n",
      "purpose.n.01\n",
      "Predicted: purpose.n.01 actual: purpose.n.01\n",
      "time.n.01\n",
      "Predicted: time.n.05 actual: time.n.01\n",
      "means.n.01\n",
      "Predicted: means.n.02 actual: means.n.01\n",
      "manner.n.01\n",
      "Predicted: manner.n.01 actual: manner.n.01\n",
      "frequency.n.01\n",
      "Predicted: frequency.n.01 actual: frequency.n.01\n",
      " None\n",
      "history.n.02\n",
      "Predicted: history.n.05 actual: history.n.02\n",
      "topic.n.02\n",
      "Predicted: topic.n.02 actual: topic.n.02\n",
      "sphere.n.01\n",
      "Predicted: knowledge_domain.n.01 actual: sphere.n.01\n",
      "end.n.02\n",
      "start.n.01\n",
      "span.n.01\n",
      "duration.n.01\n",
      "Predicted: duration.n.01 actual: duration.n.01\n",
      "time.n.01\n",
      "Predicted: time.n.05 actual: time.n.01\n",
      "Accuracy: 0.40476190476190477\n"
     ]
    }
   ],
   "source": [
    "current_frame_id = None\n",
    "\n",
    "totale = 0\n",
    "corretti = 0\n",
    "for index, row in corpus.iterrows():\n",
    "    try:\n",
    "        key = row['word'].replace(' ', '_')\n",
    "\n",
    "        if row['fn_tag'] == 'FN': #frame name\n",
    "            frame = fn.frame_by_name(row['word'])\n",
    "            current_frame_id = frame.ID\n",
    "            context = ctx_frame_name_fn(frame)\n",
    "        elif row['fn_tag'] == 'FE': #frame element\n",
    "            frame = fn.frame_by_id(current_frame_id)\n",
    "            context = ctx_frame_element_fn(frame.FE[row['word']])\n",
    "        elif row['fn_tag'] == 'LU': #lexical unit\n",
    "            frame = fn.frame_by_id(current_frame_id)\n",
    "            context = ctx_lexical_unit_fn(frame.lexUnit[row['word']])\n",
    "\n",
    "        if row['syn'] != 'None':\n",
    "            predicted_syn = bag_of_words2(key, context)\n",
    "\n",
    "            if predicted_syn is not None:\n",
    "                actual_syn = wn.synset(row['syn'])\n",
    "\n",
    "                \n",
    "                #check if the predicted synset is correct\n",
    "                print('Predicted:', predicted_syn.name(), 'actual:',  actual_syn.name())\n",
    "                if predicted_syn.name() == actual_syn.name():\n",
    "                    corretti += 1\n",
    "            \n",
    "            totale += 1\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "\n",
    "print('Accuracy:', corretti/totale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
