{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: annotazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame OLIVERio:\n",
    "[ ] Concessive\n",
    "[ ] History\n",
    "[ ] Change_resistance\n",
    "[ ] Emptying\n",
    "[ ] Performers_and_roles\n",
    "\n",
    "Frame TOMATIS\n",
    "[x] Deciding\n",
    "[x] Intentionally_act\n",
    "[x] Competition\n",
    "[x] Fairness_evaluation\n",
    "[x] Process_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definizione frame: A recordable sequence of events is associated with a given Topic.   'And that is where the history of art begins.' \n",
      "\n",
      "\n",
      "Topic :  The Topic is the subject about which the history is given.  'The role of the Byzantine Empire in European history is not understood by all.'  \n",
      "Domain :  The Domain is (a characterization of) the type of event that is to be included in consideration of the Topic's history.  'Elizabethan Stage remain classics of literary history.'  \n",
      "End_time :  The end of the span of time during which the Topic occurs. \n",
      "Start_time :  The beginning of the time span over which the Topic occurs.  \n",
      "Time_span :  The span of time over which the events related to the Topic take place.  \n",
      "Duration :  The Duration, often expressed in calendric units, of the history of the Topic.  \n",
      "Time :  When the events concerning the Topic occured.\n",
      "\n",
      "\n",
      "history.n :  FN: the sum of events associated with a given topic\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "frame = fn.frame_by_name('History')\n",
    "\n",
    "print('definizione frame:', frame.definition)\n",
    "print('\\n')\n",
    "\n",
    "for fe in frame.FE:\n",
    "    print(fe, ': ', frame.FE[fe].definition)\n",
    "\n",
    "print('\\n')\n",
    "for lu in frame.lexUnit:\n",
    "    print(lu, ': ', frame.lexUnit[lu].definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Synset('history.n.01')\n",
      "definition: the aggregate of past events\n",
      "a critical time in the school's history\n",
      "\n",
      "\n",
      "Synset('history.n.02')\n",
      "definition: a record or narrative description of past events\n",
      "a history of France\n",
      "he gave an inaccurate account of the plot to kill the president\n",
      "the story of exposure to lead\n",
      "\n",
      "\n",
      "Synset('history.n.03')\n",
      "definition: the discipline that records and interprets past events involving human beings\n",
      "he teaches Medieval history\n",
      "history takes the long view\n",
      "\n",
      "\n",
      "Synset('history.n.04')\n",
      "definition: the continuum of events occurring in succession leading from the past to the present and even into the future\n",
      "all of human history\n",
      "\n",
      "\n",
      "Synset('history.n.05')\n",
      "definition: all that is remembered of the past as preserved in writing; a body of knowledge\n",
      "the dawn of recorded history\n",
      "from the beginning of history\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "for syn in wn.synsets('history'):\n",
    "    print(syn)\n",
    "    print('definition:' , syn.definition())\n",
    "    #print example\n",
    "    for example in syn.examples():\n",
    "        print(example)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: mapping automatico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "deciding_frame = fn.frame_by_name(\"Deciding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'going', 'im', 'new_york', 'park'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creazione tokenizzatore per le multiword expressions\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "\n",
    "def make_set(sentence):\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence) #remove punctuation\n",
    "    sentence = tokenizer.tokenize(sentence.split()) #tokenize\n",
    "    sentence = [w for w in sentence if not w.isdigit()] #remove numbers\n",
    "    stop_words = set(stopwords.words('english')) #remove stop words\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer() #lemmatization of definition\n",
    "    sentence = [lemmatizer.lemmatize(w) for w in sentence]\n",
    "\n",
    "    res = []\n",
    "    for w in sentence:\n",
    "        res.append(w.replace(' ', '_'))\n",
    "\n",
    "    return set(res)\n",
    "\n",
    "make_set(\"I'm going to the park of New York.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_frame_FN(frame):\n",
    "    name = frame.name\n",
    "    definition = frame.definition\n",
    "    FEs = frame.FE\n",
    "    LUs = frame.lexUnit\n",
    "\n",
    "    sentence = name + ' ' + definition\n",
    "    for fe in FEs:\n",
    "        sentence += ' ' + FEs[fe].definition\n",
    "    for lu in LUs:\n",
    "        sentence += ' ' + LUs[lu].definition\n",
    "\n",
    "    return make_set(sentence)\n",
    "\n",
    "def ctx_frame_element_FN(frame_element):\n",
    "    name = frame_element.name\n",
    "    definition = frame_element.definition\n",
    "    #semtype = frame_element.semType.name\n",
    "\n",
    "    sentence = name + ' ' + definition # + ' ' + semtype\n",
    "   \n",
    "    return make_set(sentence)\n",
    "\n",
    "def ctx_lexical_unit_FN(lexical_unit):\n",
    "    name = lexical_unit.name\n",
    "    definition = lexical_unit.definition.split(':')[1]\n",
    "    exemplars = lexical_unit.exemplars\n",
    "    \n",
    "    sentence = name + ' ' + definition\n",
    "    for ex in exemplars:\n",
    "        sentence += ' ' + ex.annotationSet[0].text\n",
    "\n",
    "    return make_set(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione del contesto (FrameNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_FN = {}\n",
    "\n",
    "#frame\n",
    "ctx_FN[deciding_frame.name.replace(' ', '_')] = ctx_frame_FN(deciding_frame)\n",
    "\n",
    "#FEs\n",
    "for fe in deciding_frame.FE:\n",
    "    ctx_FN[fe.replace(' ', '_')] = ctx_frame_element_FN(deciding_frame.FE[fe])\n",
    "    break\n",
    "\n",
    "#LUs\n",
    "for lu in deciding_frame.lexUnit:\n",
    "    ctx_FN[lu.replace(' ', '_')] = ctx_lexical_unit_FN(deciding_frame.lexUnit[lu])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crezione del contesto (WordNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(synset):\n",
    "    hyponyms = set()\n",
    "    for hyponym in synset.hyponyms():\n",
    "        hyponyms |= set(get_hyponyms(hyponym))\n",
    "    return hyponyms | set(synset.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(synset, depth=1):\n",
    "    sentence = synset.definition()\n",
    "    for example in synset.examples():\n",
    "        sentence += ' ' + example\n",
    "    for lemma in synset.lemmas():\n",
    "        sentence += ' ' + lemma.name()\n",
    "\n",
    "    if (depth >= 0):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            sentence += ' ' + create_sentences(hypernym, depth-1)\n",
    "        for hyponym in get_hyponyms(synset):\n",
    "            sentence += ' ' + create_sentences(hyponym, depth-1)  \n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def ctx_synset_WN(synset):\n",
    "    sentence = create_sentences(synset)\n",
    "    return make_set(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deciding': Synset('decision_making.n.01'),\n",
       " 'decide.v': Synset('decide.v.02'),\n",
       " 'decision.n': Synset('decision.n.01'),\n",
       " 'rule_out.v': Synset('rule_out.v.02'),\n",
       " 'determine.v': Synset('determine.v.02')}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings = {}\n",
    "for key in ctx_FN:\n",
    "    token = key.split('.')[0]\n",
    "    syns = wn.synsets(token)\n",
    "    max_overlap = 0\n",
    "    for syn in syns:\n",
    "        overlap = len(ctx_synset_WN(syn).intersection(ctx_FN[key])) + 1\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            mappings[key] = syn\n",
    "\n",
    "mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "deciding_frame = fn.frame_by_name(\"Deciding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cognizer\n",
      "Decision\n",
      "Possibilities\n",
      "Time\n",
      "Place\n",
      "Manner\n",
      "Inherent_purpose\n",
      "Circumstance\n",
      "Explanation\n"
     ]
    }
   ],
   "source": [
    "#FEs\n",
    "for fe in deciding_frame.FE: \n",
    "    ctx_FN[fe.replace(' ', '_')] = ctx_frame_element_FN(deciding_frame.FE[fe])\n",
    "#LUs\n",
    "for lu in deciding_frame.lexUnit:\n",
    "    ctx_FN[lu.replace(' ', '_')] = ctx_lexical_unit_FN(deciding_frame.lexUnit[lu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_between_synsets(synset1, synset2, L=3):\n",
    "    paths = []\n",
    "    visited = set()\n",
    "\n",
    "    def dfs(synset, path):\n",
    "        if synset in visited or len(path) > L:\n",
    "            return\n",
    "        if synset == synset2:\n",
    "            paths.append(path + [synset])\n",
    "            return\n",
    "        visited.add(synset)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            dfs(hypernym, path + [synset])\n",
    "        for hyponym in synset.hyponyms():\n",
    "            dfs(hyponym, path + [synset])\n",
    "\n",
    "    dfs(synset1, [])\n",
    "    return [path for path in paths if len(path) <= L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('decision_making.n.01'), Synset('decide.v.01'), Synset('decide.v.02'), Synset('decide.v.03'), Synset('decide.v.04'), Synset('deciding.s.01')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Synset('decide.v.01')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score(syn_FN, word_FN, ctx_FN):\n",
    "    ctx_word = ctx_FN[word_FN]\n",
    "\n",
    "    res = 0\n",
    "    for word in ctx_word:\n",
    "        for syn in wn.synsets(word):\n",
    "            #get all connection path between syn and syn_FN\n",
    "            paths = list(get_paths_between_synsets(syn, syn_FN, 3))\n",
    "\n",
    "            for path in paths:\n",
    "                res += np.exp(-len(path)-1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def prob(syn_FN, word_FN, ctx_FN):\n",
    "    sum = 0\n",
    "    for key in ctx_FN:\n",
    "        for syn in wn.synsets(key):\n",
    "            sum += score(syn, key, ctx_FN)\n",
    "\n",
    "    return score(syn_FN, word_FN, ctx_FN) / sum\n",
    "\n",
    "def argmax_prob(word_FN, ctx_FN):\n",
    "    max_prob = 0\n",
    "    max_syn = None\n",
    "    for syn in wn.synsets(word_FN):\n",
    "        prob_syn = prob(syn, word_FN, ctx_FN)\n",
    "        if prob_syn > max_prob:\n",
    "            max_prob = prob_syn\n",
    "            max_syn = syn\n",
    "            \n",
    "    return max_syn\n",
    "\n",
    "print(wn.synsets('deciding'))\n",
    "\n",
    "argmax_prob('Deciding', ctx_FN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
