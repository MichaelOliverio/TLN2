{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame annotati:\n",
    "- Frame OLIVERio:\n",
    "    - [x] Concessive\n",
    "    - [x] History\n",
    "    - [x] Change_resistance\n",
    "    - [x] Emptying\n",
    "    - [x] Performers_and_roles\n",
    "    \n",
    "\n",
    "- Frame TOMATIS\n",
    "    - [x] Deciding\n",
    "    - [x] Intentionally_act\n",
    "    - [x] Competition\n",
    "    - [x] Fairness_evaluation\n",
    "    - [x] Process_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer #tiene conto delle multiword expressions\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione tokenizzatore per le multiword expressions\n",
    "mwes = [x for x in wn.all_lemma_names() if '_' in x]\n",
    "mwes = [tuple(x.split('_')) for x in mwes]\n",
    "tokenizer = MWETokenizer(mwes, separator=' ')\n",
    "\n",
    "def make_set(sentence):\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence) #remove punctuation\n",
    "    sentence = tokenizer.tokenize(sentence.split()) #tokenize\n",
    "    sentence = [w for w in sentence if not w.isdigit()] #remove numbers\n",
    "    stop_words = set(stopwords.words('english')) #remove stop words\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer() #lemmatization of definition\n",
    "    sentence = [lemmatizer.lemmatize(w) for w in sentence]\n",
    "\n",
    "    res = []\n",
    "    for w in sentence:\n",
    "        res.append(w.replace(' ', '_'))\n",
    "\n",
    "    return set(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodi per la creazione dei contesti (framenet e wordnet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione contesto elementi framenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_frame_name_fn(frame):\n",
    "    name = frame.name\n",
    "    definition = frame.definition\n",
    "    FEs = frame.FE\n",
    "    LUs = frame.lexUnit\n",
    "\n",
    "    sentence = name + ' ' + definition\n",
    "    for fe in FEs:\n",
    "        sentence += ' ' + FEs[fe].definition\n",
    "    for lu in LUs:\n",
    "        sentence += ' ' + LUs[lu].definition\n",
    "\n",
    "    return make_set(sentence)\n",
    "\n",
    "def ctx_frame_element_fn(frame_element):\n",
    "    name = frame_element.name\n",
    "    definition = frame_element.definition\n",
    "    #semtype = frame_element.semType.name\n",
    "\n",
    "    sentence = name + ' ' + definition # + ' ' + semtype\n",
    "   \n",
    "    return make_set(sentence)\n",
    "\n",
    "def ctx_lexical_unit_fn(lexical_unit):\n",
    "    name = lexical_unit.name\n",
    "    definition = lexical_unit.definition\n",
    "    exemplars = lexical_unit.exemplars\n",
    "    \n",
    "    sentence = name + ' ' + definition\n",
    "    for ex in exemplars:\n",
    "        sentence += ' ' + ex.annotationSet[0].text\n",
    "\n",
    "    return make_set(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crezione contesto del synset wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(synset):\n",
    "    hyponyms = set()\n",
    "    for hyponym in synset.hyponyms():\n",
    "        hyponyms |= set(get_hyponyms(hyponym))\n",
    "    return hyponyms | set(synset.hyponyms())\n",
    "\n",
    "def create_sentences(synset, depth=1):\n",
    "    sentence = synset.definition()\n",
    "    for example in synset.examples():\n",
    "        sentence += ' ' + example\n",
    "    for lemma in synset.lemmas():\n",
    "        sentence += ' ' + lemma.name()\n",
    "\n",
    "    if (depth >= 0):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            sentence += ' ' + create_sentences(hypernym, depth-1)\n",
    "        for hyponym in get_hyponyms(synset):\n",
    "            sentence += ' ' + create_sentences(hyponym, depth-1)  \n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def ctx_synset_WN(synset):\n",
    "    sentence = create_sentences(synset)\n",
    "    return make_set(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio bag of words\n",
    "\n",
    "Scelta del senso che permette di massimizzare l'intersezione tra i contesti. Non vengono associati i synset a tutti i token perchè alcuni non hanno alcun synset su wordnet (es. if, although, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(key, ctx_fn):\n",
    "    best_syn = None\n",
    "\n",
    "    token = key.split('.')[0]\n",
    "    syns = wn.synsets(token)\n",
    "    max_overlap = 0\n",
    "\n",
    "    # dati i synset andiamo a prendere quello più accurato con il frame\n",
    "    for syn in syns:\n",
    "        overlap = len(ctx_synset_WN(syn).intersection(ctx_fn)) + 1\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_syn = syn\n",
    "\n",
    "    return best_syn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_between_synsets(synset1, synset2, L=3):\n",
    "    paths = []\n",
    "    visited = set()\n",
    "\n",
    "    def dfs(synset, path):\n",
    "        if synset in visited or len(path) > L:\n",
    "            return\n",
    "        if synset == synset2:\n",
    "            paths.append(path + [synset])\n",
    "            return\n",
    "        visited.add(synset)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            dfs(hypernym, path + [synset])\n",
    "        for hyponym in synset.hyponyms():\n",
    "            dfs(hyponym, path + [synset])\n",
    "\n",
    "    dfs(synset1, [])\n",
    "    return [path for path in paths if len(path) <= L]\n",
    "\n",
    "def score(syn_fn, word_fn, ctx_fn):\n",
    "    res = 0\n",
    "    for word in ctx_fn:\n",
    "        for syn in wn.synsets(word):\n",
    "            #get all connection path between syn and syn_fn\n",
    "            paths = list(get_paths_between_synsets(syn, syn_fn, 3))\n",
    "\n",
    "            for path in paths:\n",
    "                res += np.exp(-len(path)-1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def prob(syn_fn, word_fn, ctx_fn):\n",
    "    sum = 0\n",
    "\n",
    "    for syn in wn.synsets(word_fn):\n",
    "        sum += score(syn, word_fn, ctx_fn)\n",
    "\n",
    "    return score(syn_fn, word_fn, ctx_fn) / sum\n",
    "\n",
    "def argmax_prob(word_fn, ctx_fn):\n",
    "    max_prob = 0\n",
    "    max_syn = None\n",
    "    for syn in wn.synsets(word_fn):\n",
    "        prob_syn = prob(syn, word_fn, ctx_fn)\n",
    "        if prob_syn > max_prob:\n",
    "            max_prob = prob_syn\n",
    "            max_syn = syn\n",
    "            \n",
    "    return max_syn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione\n",
    "\n",
    "La funzionalità di valutazione confronta i synset restituiti in output dal sistema con quelli annotati a mano; su questa base deve essere calcolata l'accuratezza del sistema, semplicemente come rapporto degli elementi corretti sul totale degli elementi.\n",
    "\n",
    "Opzionale\n",
    "- Confronto fra l'output dei due approcci descritti (bag-of-words e con grafo).\n",
    "- Sviluppo di metriche che considerino anche la distanza semantica fra eventuali synset errati e corretti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(corpus, method='bag_of_words'):\n",
    "    current_frame_id = None\n",
    "    totale = 0\n",
    "    corretti = 0\n",
    "    \n",
    "    for index, row in corpus.iterrows():\n",
    "        try:\n",
    "            key = row['word'].replace(' ', '_')\n",
    "\n",
    "            if row['fn_tag'] == 'FN': #frame name\n",
    "                frame = fn.frame_by_name(row['word'])\n",
    "                current_frame_id = frame.ID\n",
    "                context = ctx_frame_name_fn(frame)\n",
    "            elif row['fn_tag'] == 'FE': #frame element\n",
    "                frame = fn.frame_by_id(current_frame_id)\n",
    "                context = ctx_frame_element_fn(frame.FE[row['word']])\n",
    "            elif row['fn_tag'] == 'LU': #lexical unit\n",
    "                frame = fn.frame_by_id(current_frame_id)\n",
    "                context = ctx_lexical_unit_fn(frame.lexUnit[row['word']])\n",
    "\n",
    "            if row['syn'] != 'None':\n",
    "                predicted_syn = None\n",
    "                \n",
    "                if method == 'bag_of_words':\n",
    "                    predicted_syn = bag_of_words(key, context)\n",
    "                elif method == 'approccio_grafico':\n",
    "                    predicted_syn = argmax_prob(key, context)\n",
    "\n",
    "                if predicted_syn is not None:\n",
    "                    actual_syn = wn.synset(row['syn'])\n",
    "\n",
    "                    #check if the predicted synset is correct\n",
    "                    #print('Predicted:', predicted_syn.name(), 'actual:',  actual_syn.name())\n",
    "                    if predicted_syn.name() == actual_syn.name():\n",
    "                        corretti += 1\n",
    "                \n",
    "                totale += 1\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "    return corretti/totale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OliverioM\\AppData\\Local\\Temp\\ipykernel_14460\\31125095.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  corpus = pd.read_csv('frame_annotati.csv', error_bad_lines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation...\n",
      "Accuracy bag of words: 0.40476190476190477\n",
      "Evaluation...\n",
      "Accuracy approccio grafico: 0.30120481927710846\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv('frame_annotati.csv', error_bad_lines=False)\n",
    "\n",
    "print('Accuracy bag of words:', evaluation(corpus, method='bag_of_words'))\n",
    "print('Accuracy approccio grafico:', evaluation(corpus, method='approccio_grafico'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
