{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 2\n",
    "\n",
    "Implementare l’algoritmo di Lesk (!= usare implementazione esistente, e.g., in nltk…).\n",
    "- Estrarre 50 frasi dal corpus SemCor (corpus annotato con i synset di WN) e disambiguare (almeno) un sostantivo per frase. Calcolare l’accuratezza del sistema implementato sulla base dei sensi annotati in SemCor (SemCor è disponibile all’URL http://web.eecs.umich.edu/~mihalcea/downloads.html)\n",
    "- Randomizzare la selezione delle 50 frasi e la selezione del termine da disambiguare, e restituire l’accuratezza media su (per esempio) 10 esecuzioni del programma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "#nltk.('sedownloadmcor')\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_list(sent):\n",
    "    return [l.label() if isinstance(l, nltk.tree.Tree) else None for l in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimplifiedLesk(word, sentence):\n",
    "    best_sense = wn.synsets(word)[0] if len(wn.synsets(word)) > 0 else None\n",
    "    max_overlap = 0\n",
    "    context = set(sentence)\n",
    "    \n",
    "    for sense in wn.synsets(word):\n",
    "        signature = set(sense.definition().split()).union(set(sense.examples()))\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divorce\n",
      "Synset('divorce.n.01')\n",
      "[Lemma('divorce.n.01.divorce')]\n",
      "-----------------\n",
      "permit\n",
      "Synset('license.n.01')\n",
      "[Lemma('license.n.01.permit')]\n",
      "-----------------\n",
      "jury\n",
      "Synset('jury.n.01')\n",
      "[Lemma('jury.n.01.jury')]\n",
      "-----------------\n",
      "item\n",
      "Synset('item.n.01')\n",
      "[Lemma('detail.n.01.item')]\n",
      "-----------------\n",
      "Felix\n",
      "None\n",
      "[Lemma('person.n.01.person')]\n",
      "-----------------\n",
      "recommend\n",
      "Synset('commend.v.04')\n",
      "[Lemma('recommend.v.01.recommend')]\n",
      "-----------------\n",
      "burden\n",
      "Synset('charge.v.18')\n",
      "[Lemma('burden.n.01.burden')]\n",
      "-----------------\n",
      "give\n",
      "Synset('give.n.01')\n",
      "[Lemma('give.v.10.give')]\n",
      "-----------------\n",
      "work\n",
      "Synset('work.n.05')\n",
      "[Lemma('work.v.01.work')]\n",
      "-----------------\n",
      "escheat\n",
      "Synset('escheat.n.01')\n",
      "[Lemma('escheat.n.01.escheat')]\n",
      "-----------------\n",
      "allotted\n",
      "Synset('accord.v.02')\n",
      "[Lemma('accord.v.02.allot')]\n",
      "-----------------\n",
      "committee\n",
      "Synset('committee.n.01')\n",
      "[Lemma('committee.n.01.committee'), Lemma('subcommittee.n.01.subcommittee')]\n",
      "-----------------\n",
      "saw\n",
      "Synset('saw.v.01')\n",
      "[Lemma('witness.v.02.see')]\n",
      "-----------------\n",
      "Fulton\n",
      "Synset('fulton.n.01')\n",
      "[Lemma('location.n.01.location'), Lemma('location.n.01.location')]\n",
      "-----------------\n",
      "believes\n",
      "Synset('believe.v.01')\n",
      "[Lemma('believe.v.03.believe')]\n",
      "-----------------\n",
      "said\n",
      "Synset('aforesaid.s.01')\n",
      "[Lemma('allege.v.01.say')]\n",
      "-----------------\n",
      "jury\n",
      "Synset('jury.n.01')\n",
      "[Lemma('jury.n.01.jury')]\n",
      "-----------------\n",
      "permit\n",
      "Synset('let.v.01')\n",
      "[Lemma('permit.v.01.permit')]\n",
      "-----------------\n",
      "wife\n",
      "Synset('wife.n.01')\n",
      "[]\n",
      "-----------------\n",
      "City\n",
      "Synset('city.n.03')\n",
      "[Lemma('group.n.01.group')]\n",
      "-----------------\n",
      "voters\n",
      "Synset('voter.n.01')\n",
      "[Lemma('voter.n.01.voter')]\n",
      "-----------------\n",
      "rescind\n",
      "Synset('revoke.v.02')\n",
      "[Lemma('revoke.v.02.rescind')]\n",
      "-----------------\n",
      "Only\n",
      "Synset('lone.s.03')\n",
      "[Lemma('merely.r.01.only')]\n",
      "-----------------\n",
      "Georgia\n",
      "Synset('georgia.n.02')\n",
      "[Lemma('georgia.n.01.Georgia')]\n",
      "-----------------\n",
      "wanted\n",
      "Synset('want.v.04')\n",
      "[Lemma('desire.v.01.want')]\n",
      "-----------------\n",
      "Highway\n",
      "Synset('highway.n.01')\n",
      "[Lemma('group.n.01.group')]\n",
      "-----------------\n",
      "well\n",
      "Synset('well.n.04')\n",
      "[Lemma('well.r.01.well')]\n",
      "-----------------\n",
      "representing\n",
      "Synset('represent.v.04')\n",
      "[Lemma('represent.v.03.represent')]\n",
      "-----------------\n",
      "controversy\n",
      "Synset('controversy.n.01')\n",
      "[Lemma('controversy.n.01.controversy')]\n",
      "-----------------\n",
      "Davis\n",
      "Synset('davis.n.05')\n",
      "[Lemma('person.n.01.person')]\n",
      "-----------------\n",
      "Marvin\n",
      "None\n",
      "[Lemma('person.n.01.person')]\n",
      "-----------------\n",
      "trouble\n",
      "Synset('trouble.n.01')\n",
      "[Lemma('fuss.n.02.trouble')]\n",
      "-----------------\n",
      "property\n",
      "Synset('property.n.05')\n",
      "[Lemma('property.n.01.property')]\n",
      "-----------------\n",
      "A\n",
      "Synset('deoxyadenosine_monophosphate.n.01')\n",
      "[None]\n",
      "-----------------\n",
      "become\n",
      "Synset('become.v.01')\n",
      "[Lemma('become.v.02.become')]\n",
      "-----------------\n",
      "mental\n",
      "Synset('mental.a.01')\n",
      "[Lemma('mental.a.02.mental')]\n",
      "-----------------\n",
      "recommendations\n",
      "Synset('recommendation.n.01')\n",
      "[Lemma('recommendation.n.01.recommendation')]\n",
      "-----------------\n",
      "issue\n",
      "Synset('issue.n.04')\n",
      "[Lemma('publish.v.02.issue')]\n",
      "-----------------\n",
      "would\n",
      "None\n",
      "[None, None]\n",
      "-----------------\n",
      "After\n",
      "Synset('after.r.02')\n",
      "[None]\n",
      "-----------------\n",
      "Committee\n",
      "Synset('committee.n.01')\n",
      "[Lemma('group.n.01.group')]\n",
      "-----------------\n",
      "roads\n",
      "Synset('road.n.02')\n",
      "[Lemma('road.n.01.road')]\n",
      "-----------------\n",
      "picking\n",
      "Synset('picking.n.01')\n",
      "[Lemma('turn_around.v.03.pick_up')]\n",
      "-----------------\n",
      "enthusiastic\n",
      "Synset('enthusiastic.a.01')\n",
      "[Lemma('enthusiastic.a.01.enthusiastic')]\n",
      "-----------------\n",
      "many\n",
      "Synset('many.a.01')\n",
      "[None]\n",
      "-----------------\n",
      "County\n",
      "Synset('county.n.01')\n",
      "[Lemma('location.n.01.location')]\n",
      "-----------------\n",
      "awarding\n",
      "Synset('award.v.02')\n",
      "[Lemma('award.n.01.awarding')]\n",
      "-----------------\n",
      "enter\n",
      "Synset('enter.v.02')\n",
      "[Lemma('enroll.v.01.enter')]\n",
      "-----------------\n",
      "expires\n",
      "Synset('run_out.v.04')\n",
      "[Lemma('run_out.v.04.expire')]\n",
      "-----------------\n",
      "It\n",
      "Synset('information_technology.n.01')\n",
      "[None]\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "#get 50 random sentences from semcor\n",
    "sentences_with_tag = semcor.tagged_sents(tag = 'sem')[:100]\n",
    "sentences = semcor.sents()[:100]\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(sentences_with_tag))\n",
    "indices = indices[:50]\n",
    "\n",
    "my_sentences_with_tag = [sentences_with_tag[i] for i in indices]\n",
    "my_sentences = [sentences[i] for i in indices]\n",
    "\n",
    "#print(my_sentences_with_tag[0])\n",
    "#print(my_sentences[0])\n",
    "\n",
    "for i, sentence in enumerate(my_sentences):\n",
    "    #lowercase\n",
    "    #sentence = [w.lower() for w in sentence]\n",
    "    #remove stop words\n",
    "    sentence_without_stopwords = [w for w in sentence if not w in stop_words]\n",
    "    #remove punctuation\n",
    "    sentence_without_stopwords = [w for w in sentence_without_stopwords if w.isalpha()]\n",
    "    #get random word\n",
    "    random_word = random.choice(sentence_without_stopwords)\n",
    "    index = sentence.index(random_word)\n",
    "\n",
    "    #print(random_word)\n",
    "\n",
    "    actual_synset = []\n",
    "    for j, cell in enumerate(my_sentences_with_tag[i]):\n",
    "        if random_word in cell[0]:\n",
    "            actual_synset.append(cell.label() if isinstance(cell, nltk.tree.Tree) else None)\n",
    "\n",
    "    #run lesk\n",
    "    predicted_synset = SimplifiedLesk(random_word, sentence)\n",
    "    print(random_word)\n",
    "    print(predicted_synset)\n",
    "    print(actual_synset)\n",
    "    print('-----------------')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
